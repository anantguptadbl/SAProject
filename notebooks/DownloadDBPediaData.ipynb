{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "import json\n",
    "import itertools\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import threading\n",
    "\n",
    "\n",
    "class getDataFromDBPedia():\n",
    "    def __init__(self,typeString,workFolder):\n",
    "        self.typeString=typeString\n",
    "        self.workFolder=workFolder\n",
    "        self.status={}\n",
    "        self.invidualItemStatus='incomplete'\n",
    "        self.invidualItemLastLetter='AA'\n",
    "        self.invidualItemLastFile=0\n",
    "        \n",
    "        # We will write a dummy JSON status file if not already present, so that it need not be ahndled every time\n",
    "        if os.path.exists(workFolder + '/status.json'):\n",
    "            print(\"We already have a prior status\")\n",
    "        else:\n",
    "            self.status['type']=self.typeString\n",
    "            self.status['individualItemStatus']={}\n",
    "            self.status['individualItemStatus']['status']=self.invidualItemStatus\n",
    "            self.status['individualItemStatus']['lastLetter']=self.invidualItemLastLetter\n",
    "            self.status['individualItemStatus']['lastFile']=self.invidualItemLastFile\n",
    "            with open(workFolder + '/status.json','w') as f:\n",
    "                f.write(json.dumps(self.status))\n",
    "        \n",
    "    def getStatusInividualItems(self):\n",
    "        # Check whether we already have downloaded some data in the existing workfolder\n",
    "        with open(self.workFolder + '/status.json') as f:\n",
    "            self.status = json.load(f)\n",
    "        if( 'type' in self.status):\n",
    "            if(self.status['type'] != self.typeString):\n",
    "                return False\n",
    "        if( 'individualItemStatus' in self.status):\n",
    "            # This means that we have prior downloaded\n",
    "            self.invidualItemStatus=self.status['individualItemStatus']['status']\n",
    "            self.invidualItemLastLetter=self.status['individualItemStatus']['lastLetter']\n",
    "            self.invidualItemLastFile=self.status['individualItemStatus']['lastFile']\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def getIndividualItems(self):\n",
    "        firstCharacterString='ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789!@_#$^&='\n",
    "        characterList=list(itertools.product(firstCharacterString,firstCharacterString))\n",
    "        characterList=[str(x[0]) + str(x[1]) for x in characterList]\n",
    "        fullCompanyData=pd.DataFrame()\n",
    "        \n",
    "        for firstThreeCharacters in characterList[characterList.index(self.invidualItemLastLetter):]:\n",
    "            query=\"\"\"SELECT distinct ?orgObject ?orgLabel\n",
    "            WHERE {\n",
    "            ?orgObject rdf:type placeholderObjectType .\n",
    "            ?orgObject rdfs:label ?orgLabel .\n",
    "            filter langMatches(lang(?orgLabel),\"en\")\n",
    "            filter(regex(REPLACE(?orgLabel, '\"', '', \"i\"),\"^%s\",\"i\"))\n",
    "            }\n",
    "            \"\"\" % format(firstThreeCharacters)\n",
    "            query=query.replace('placeholderObjectType',self.typeString)\n",
    "            data=self.returnDF(self.get_dbpedia_sparql_data(query))\n",
    "            print(\"{} : {}\".format(firstThreeCharacters,data.shape[0]))\n",
    "            fullCompanyData=pd.concat([fullCompanyData,data])\n",
    "            if(fullCompanyData.shape[0] > 50000):\n",
    "                print(\"Writing data to file {}\".format(self.invidualItemLastFile))\n",
    "                fullCompanyData.to_csv(self.workFolder + '/FullData{}.csv'.format(self.invidualItemLastFile),index=False,encoding='utf-8')\n",
    "                self.invidualItemLastFile=self.invidualItemLastFile + 1\n",
    "                fullCompanyData=pd.DataFrame()\n",
    "                \n",
    "                # We will refresh the status JSON and write it back\n",
    "                self.invidualItemStatus='incomplete'\n",
    "                self.invidualItemLastLetter=firstThreeCharacters\n",
    "                self.status['individualItemStatus']['status']=self.invidualItemStatus\n",
    "                self.status['individualItemStatus']['lastLetter']=self.invidualItemLastLetter\n",
    "                self.status['individualItemStatus']['lastFile']=self.invidualItemLastFile\n",
    "                with open(self.workFolder + '/status.json','w') as f:\n",
    "                    f.write(json.dumps(self.status))\n",
    "        \n",
    "        print(\"Writing data to file {}\".format(self.fileCounter))\n",
    "        fullCompanyData.to_csv(self.workFolder + '/FullData{}.csv'.format(self.invidualItemLastFile),index=False,encoding='utf-8')\n",
    "        self.invidualItemStatus='complete'\n",
    "        self.invidualItemLastLetter=firstThreeCharacters\n",
    "        self.invidualItemLastFile=fileCounter\n",
    "        self.status['individualItemStatus']['status']=self.invidualItemStatus\n",
    "        self.status['individualItemStatus']['lastLetter']=self.invidualItemLastLetter\n",
    "        self.status['individualItemStatus']['lastFile']=self.invidualItemLastFile\n",
    "        with open(self.workFolder + '/status.json','w') as f:\n",
    "            f.write(json.dumps(self.status))\n",
    "\n",
    "    def get_dbpedia_sparql_data(self,query):\n",
    "        sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        sparql.setQuery(query)  # the previous query as a literal string\n",
    "        return sparql.query().convert()\n",
    "\n",
    "    def returnDF(self,data):\n",
    "        dataArray=[]\n",
    "        colNames=data['head']['vars']\n",
    "        for row in data['results']['bindings']:\n",
    "            rowArray=[]\n",
    "            for colName in colNames:\n",
    "                rowArray.append(row[colName]['value'])\n",
    "            dataArray.append(rowArray)\n",
    "\n",
    "        dataArray=pd.DataFrame(dataArray,columns=colNames)\n",
    "        return(dataArray)\n",
    "            \n",
    "if __name__==\"__main__\":\n",
    "    ThingObject=getDataFromDBPedia('owl:Thing','/home/anantgupta/Documents/Programming/dbpedia/Thing')\n",
    "    if(ThingObject.getStatusInividualItems()==True):\n",
    "        ThingObject.getIndividualItems()\n",
    "    \n",
    "# After we have got the ORG LIST, we will now be exporting the entire ORG Data Structure available in DBPEDIA\n",
    "import os\n",
    "fullData=pd.DataFrame()\n",
    "for x in os.listdir('/home/anantgupta/Documents/Programming/dbpedia/Thing'):\n",
    "    fullData=pd.concat([fullData,pd.read_csv('/home/anantgupta/Documents/Programming/dbpedia/Thing/'+str(x))])\n",
    "    print(fullData.shape)\n",
    "\n",
    "fullData.to_csv('/home/anantgupta/Documents/Programming/dbpedia/Thing/ThingFullList.txt',index=False)\n",
    "\n",
    "# Get the details around each Thing\n",
    "orgFullData=pd.read_csv('/home/anantgupta/Documents/Programming/dbpedia/Thing/ThingFullList.txt')\n",
    "\n",
    "orgDetailedData=pd.DataFrame(columns=['org','property','value'])\n",
    "batch='A'\n",
    "batchCounter=0\n",
    "for iterCounter,x in enumerate(orgFullData['orgObject'].values):\n",
    "    query=\"\"\"\n",
    "    SELECT ?property ?hasValue\n",
    "    WHERE {\n",
    "      <%s> ?property ?hasValue\n",
    "    }\n",
    "    \"\"\" % (x)\n",
    "    tempData=returnDF(get_dbpedia_sparql_data(query))\n",
    "    orgDetailedData=pd.concat([orgDetailedData,pd.DataFrame(zip([x] * tempData.shape[0],tempData['property'].values,tempData['hasValue'].values),columns=['org','property','value'])])    \n",
    "    if(iterCounter % 1000 ==0):\n",
    "        orgDetailedData.to_csv('/home/anantgupta/Documents/Programming/dbpedia/Thing/' + batch+'_'+str(batchCounter)+'.csv',index=False,encoding='utf-8')\n",
    "        print(\"Finished counter {} and batch {}\".format(batchCounter,batch))\n",
    "        orgDetailedData=pd.DataFrame(columns=['org','property','value'])\n",
    "        batchCounter=batchCounter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "import json\n",
    "import itertools\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import threading\n",
    "import os\n",
    "\n",
    "# Get the list of companies for which we need data\n",
    "def get_dbpedia_sparql_data(query):\n",
    "        sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        sparql.setQuery(query)  # the previous query as a literal string\n",
    "        return sparql.query().convert()\n",
    "\n",
    "def returnDF(data):\n",
    "    dataArray=[]\n",
    "    colNames=data['head']['vars']\n",
    "    for row in data['results']['bindings']:\n",
    "        rowArray=[]\n",
    "        for colName in colNames:\n",
    "            rowArray.append(row[colName]['value'])\n",
    "        dataArray.append(rowArray)\n",
    "    dataArray=pd.DataFrame(dataArray,columns=colNames)\n",
    "    return(dataArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writtend to file 0\n",
      "Writtend to file 1\n",
      "Writtend to file 2\n",
      "Writtend to file 3\n",
      "Writtend to file 4\n",
      "Writtend to file 5\n",
      "Writtend to file 6\n",
      "Writtend to file 7\n",
      "Written to file 8\n"
     ]
    }
   ],
   "source": [
    "# Get the list of companies which have location\n",
    "companyListQuery='select distinct ?a where {?a rdf:type <http://dbpedia.org/ontology/Company> . ?a <http://dbpedia.org/ontology/country> ?y }'\n",
    "companyList=returnDF(get_dbpedia_sparql_data(companyListQuery))\n",
    "companyList.columns=['CompanyName']\n",
    "\n",
    "# We will now get all the data for each of the company with Location\n",
    "fileCounter=0\n",
    "FullCompanyData=pd.DataFrame(columns=['property','hasValue','CompanyName'])\n",
    "for curCompanyName in companyList['CompanyName'].values:\n",
    "    companyQuery='''SELECT ?property ?hasValue\n",
    "    WHERE {\n",
    "      { <%s> ?property ?hasValue }\n",
    "      UNION\n",
    "      { ?hasValue ?property <%s> }\n",
    "    }''' % (curCompanyName,curCompanyName)\n",
    "    curCompanyData=returnDF(get_dbpedia_sparql_data(companyQuery))\n",
    "    curCompanyData['Company']=curCompanyName\n",
    "    curCompanyData.columns=['property','hasValue','CompanyName']\n",
    "    FullCompanyData=pd.concat([FullCompanyData,curCompanyData])\n",
    "    if(FullCompanyData.shape[0] > 100000):    \n",
    "        FullCompanyData.to_csv('/home/ubuntu/anant/data/CompanyDataWithLocation_{}.txt'.format(fileCounter),index=False,encoding='utf-8')\n",
    "        fileCounter=fileCounter + 1\n",
    "        FullCompanyData=pd.DataFrame(columns=['property','hasValue','CompanyName'])\n",
    "        print(\"Writtend to file {}\".format(fileCounter-1))\n",
    "\n",
    "FullCompanyData.to_csv('/home/ubuntu/anant/data/CompanyDataWithLocation_{}.txt'.format(fileCounter),index=False,encoding='utf-8')\n",
    "fileCounter=fileCounter + 1\n",
    "FullCompanyData=pd.DataFrame(columns=['property','hasValue','CompanyName'])\n",
    "print(\"Written to file {}\".format(fileCounter-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicate data for all the RHS\n",
    "predicateList=pd.read_csv('/home/ubuntu/anant/data/predicateListJoins.txt',header=-1)\n",
    "predicateList.columns=['property']\n",
    "predicateList['property']=predicateList['property'].map(lambda x : str(x).replace('<','').replace('>',''))\n",
    "\n",
    "# Complete list of all predicates\n",
    "predicateValues=[]\n",
    "\n",
    "for curFileName in os.listdir('/home/ubuntu/anant/data/'):\n",
    "    if('CompanyDataWithLocation_' in curFileName ):\n",
    "        FullCompanyData=pd.read_csv('/home/ubuntu/anant/data/{}'.format(curFileName))\n",
    "        tempValues=list(FullCompanyData.merge(predicateList,left_on='property',right_on='property',how='inner')['hasValue'].unique())\n",
    "        predicateValues=predicateValues + tempValues\n",
    "        \n",
    "predicateValues=list(set(predicateValues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for Stock photos\n",
      "Error for Construction & Industrial Equipment Rentals and Sales\n",
      "Error for Centro Watt Trust\n",
      "Error for GPS tracking devices\n",
      "Error for Pet station refill bag rolls, Individual resident scoop bag dispensers,\n",
      "Written to file 0 with shape 106150\n",
      "Error for Stock video, audio and graphics\n",
      "Error for EDMBiz conference, EDM festivals, music publishing\n",
      "Written to file 1 with shape 103914\n",
      "Error for Documents  Membership, Home Equity, Business\n",
      "Error for oil and gas\n",
      "Error for Life insurance\n",
      "Written to file 2 with shape 106419\n",
      "Error for Centro Direct Property Fund International\n",
      "Written to file 3 with shape 109791\n",
      "Error for No Yolk noodles, Wacky Mac Pasta, Wacky Mac & Cheese\n",
      "Error for Recommendation Engine\n",
      "Written to file 4 with shape 107846\n",
      "Written to file 5 with shape 101556\n",
      "Error for Web services\n",
      "Error for ShoutMD, Teoxane, PRN Physician Recommended Nutriceutials, Skin Essentials, Alphaeon Beauty, AlphaeonMD, iTrace\n",
      "Written to file 6 with shape 100194\n",
      "Error for Centro Direct Property Fund\n",
      "Written to file 7 with shape 100714\n",
      "Written to file 8 with shape 106676\n",
      "Error for Site Search Analytics\n",
      "Written to file 9 with shape 104454\n",
      "Written to file 10 with shape 102199\n",
      "Written to file 11 with shape 103390\n",
      "Error for soybean and corn genetics\n",
      "Error for Public company\n",
      "Error for RealtySouth, TitleSouth, Home Service Lending, RealtySouth Relocation Services, InsuranceSouth\n",
      "Error for Provided security primarily to shopping centers\n",
      "Error for Centro Wholesale Funds\n",
      "Error for CompressorsExpandersEngines Air Conditioners Integrated Motors/Generators\n",
      "Written to file 12 with shape 100049\n",
      "Written to file 13 with shape 103762\n",
      "Written to file 14 with shape 103784\n",
      "Written to file 15 with shape 108174\n",
      "Written to file 16 with shape 104776\n",
      "Error for Higher education services\n",
      "Error for Centro Retail Trust\n",
      "Written to file 17 with shape 106503\n",
      "Written to file 18 with shape 102876\n",
      "Written to file 19 with shape 107983\n",
      "Error for Key Phrase Extractor\n",
      "Written to file 20 with shape 104750\n",
      "Error for DYM ReSearcher\n",
      "Written to file 21 with shape 101924\n",
      "Error for Obi Lifespeed\n",
      "Written to file 22 with shape 109511\n",
      "Error for SPM Performance Monitoring\n",
      "Written to file 23 with shape 104765\n",
      "Written to file 24 with shape 107239\n",
      "Error for Employee benefits\n",
      "Written to file 25 with shape 106250\n",
      "Error for Bakery, dairy, deli, frozen foods, general grocery, meat, produce, snacks\n",
      "Error for Obi SF1\n",
      "Written to file 26 with shape 108705\n",
      "Error for Telecommunications devices\n",
      "Error for Innovation, brand strategy, design, culture change\n",
      "Written to file 27 with shape 104902\n",
      "Error for Retirement plans\n",
      "Written to file 28 with shape 104936\n",
      "Error for Online software\n",
      "Written to file 29 with shape 101523\n",
      "Written to file 30 with shape 109269\n",
      "Error for Movies, Television shows, Music, Movie theaters\n",
      "Error for Logsene Log Analytics\n",
      "Error for Critical communications, safety and workflow solutions for schools and hospitals\n",
      "Written to file 31 with shape 102255\n",
      "Written to file 32 with shape 107273\n",
      "Written to file 33 with shape 107159\n",
      "Written to file 34 with shape 100050\n",
      "Error for grocery, soft drinks, prepared foods\n",
      "Error for Obi SJ1.5\n",
      "Written to file 35 with shape 104493\n",
      "Error for Mobile apps\n",
      "Written to file 36 with shape 101480\n",
      "Error for Centro MCS Syndicates\n",
      "Written to file 37 with shape 100831\n",
      "Error for Rob Caves\n",
      "Error for Obi MV1\n",
      "Error for Reference handbooks\n",
      "Written to file 38 with shape 102438\n",
      "Written to file 39 with shape 102731\n",
      "Error for Guaranteed Networks, Guaranteed Networks on Demand, Network Assurance\n",
      "Error for Exel North America specializes in finishing and dispensing.  It manufactures equipment such as manual and automatic paint spray guns, pumps, dispense heads, plural component equipment for finishing and dispensing.\n",
      "Error for Asset Vision, WinINSTALL, Survey\n",
      "Written to file 40 with shape 104054\n",
      "Written to file 41 with shape 43010\n"
     ]
    }
   ],
   "source": [
    "predicateData=pd.DataFrame(columns=['property','hasValue','Predicate'])\n",
    "fileCounter=0\n",
    "\n",
    "for curPredicate in predicateValues:\n",
    "    predicateQuery='''SELECT ?property ?hasValue\n",
    "    WHERE {\n",
    "      { <%s> ?property ?hasValue }\n",
    "      UNION\n",
    "      { ?hasValue ?property <%s> }\n",
    "    }''' % (curPredicate,curPredicate)\n",
    "    try:\n",
    "        curpredicateData=returnDF(get_dbpedia_sparql_data(predicateQuery))\n",
    "        curpredicateData['Predicate']=curPredicate\n",
    "        curpredicateData.columns=['property','hasValue','Predicate']\n",
    "        predicateData=pd.concat([predicateData,curpredicateData])\n",
    "    except:\n",
    "        print(\"Error for {}\".format(curPredicate))\n",
    "    if(predicateData.shape[0] > 100000):    \n",
    "        predicateData.to_csv('/home/ubuntu/anant/data/PredicateDataWithLocation_{}.txt'.format(fileCounter),index=False,encoding='utf-8')\n",
    "        fileCounter=fileCounter + 1\n",
    "        print(\"Written to file {} with shape {}\".format(fileCounter-1,predicateData.shape[0]))\n",
    "        predicateData=pd.DataFrame(columns=['property','hasValue','Predicate'])\n",
    "\n",
    "predicateData.to_csv('/home/ubuntu/anant/data/PredicateDataWithLocation_{}.txt'.format(fileCounter),index=False,encoding='utf-8')\n",
    "fileCounter=fileCounter + 1\n",
    "print(\"Written to file {} with shape {}\".format(fileCounter-1,predicateData.shape[0]))\n",
    "predicateData=pd.DataFrame(columns=['property','hasValue','Predicate'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
