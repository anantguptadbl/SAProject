{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "import json\n",
    "import itertools\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import threading\n",
    "\n",
    "\n",
    "# Get the list of companies for which we need data\n",
    "def get_dbpedia_sparql_data(query):\n",
    "        sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        sparql.setQuery(query)  # the previous query as a literal string\n",
    "        return sparql.query().convert()\n",
    "\n",
    "def returnDF(data):\n",
    "    dataArray=[]\n",
    "    colNames=data['head']['vars']\n",
    "    for row in data['results']['bindings']:\n",
    "        rowArray=[]\n",
    "        for colName in colNames:\n",
    "            rowArray.append(row[colName]['value'])\n",
    "        dataArray.append(rowArray)\n",
    "    dataArray=pd.DataFrame(dataArray,columns=colNames)\n",
    "    return(dataArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9130\n",
      "16504\n",
      "26454\n",
      "31230\n",
      "35562\n",
      "40055\n",
      "44825\n",
      "49077\n",
      "52834\n",
      "55016\n",
      "58030\n",
      "62353\n",
      "69826\n",
      "73904\n",
      "76384\n",
      "82887\n",
      "83386\n",
      "87991\n",
      "97991\n",
      "105294\n",
      "107028\n",
      "109666\n",
      "113017\n",
      "113318\n",
      "113956\n",
      "114805\n",
      "114810\n",
      "114964\n",
      "115117\n",
      "115221\n",
      "115281\n",
      "115323\n",
      "115343\n",
      "115367\n",
      "115393\n",
      "115416\n",
      "115416\n",
      "115423\n",
      "115423\n",
      "115423\n",
      "115423\n",
      "125423\n",
      "125424\n",
      "125424\n",
      "The total company List is 125424\n"
     ]
    }
   ],
   "source": [
    "# We will now get all the data for each of the company irrespective of location\n",
    "firstCharacterString='ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789!@_#$^&='\n",
    "companyListAllLocations=pd.DataFrame(columns=['CompanyName'])\n",
    "for curCharacter in firstCharacterString:\n",
    "    curCompanyListQueryAllLocations='''\n",
    "    SELECT distinct ?a\n",
    "    WHERE { ?a rdf:type <http://dbpedia.org/ontology/Company> .\n",
    "    ?a rdfs:label ?aLabel\n",
    "    filter(regex(REPLACE(?aLabel, '\"', '', \"i\"),\"^%s\",\"i\"))\n",
    "    }\n",
    "    ''' % format(curCharacter)\n",
    "    curCompanyListAllLocations=returnDF(get_dbpedia_sparql_data(curCompanyListQueryAllLocations))\n",
    "    curCompanyListAllLocations.columns=['CompanyName']\n",
    "    companyListAllLocations=pd.concat([companyListAllLocations,curCompanyListAllLocations])\n",
    "    print(companyListAllLocations.shape[0])\n",
    "    \n",
    "print(\"The total company List is {}\".format(companyListAllLocations.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written to file 0\n",
      "Written to file 1\n",
      "Written to file 2\n",
      "Written to file 3\n",
      "Written to file 4\n",
      "Written to file 5\n",
      "Written to file 6\n",
      "Written to file 7\n",
      "Written to file 8\n",
      "Written to file 9\n",
      "Written to file 10\n",
      "Written to file 11\n",
      "Written to file 12\n",
      "Written to file 13\n",
      "Written to file 14\n",
      "Written to file 15\n",
      "Written to file 16\n",
      "Written to file 17\n",
      "Written to file 18\n",
      "Written to file 19\n",
      "Written to file 20\n",
      "Written to file 21\n",
      "Written to file 22\n",
      "Written to file 23\n",
      "Written to file 24\n",
      "Written to file 25\n",
      "Written to file 26\n",
      "Written to file 27\n",
      "Written to file 28\n",
      "Written to file 29\n",
      "Written to file 30\n",
      "Written to file 31\n",
      "Written to file 32\n",
      "Written to file 33\n",
      "Written to file 34\n",
      "Written to file 35\n",
      "Written to file 36\n"
     ]
    }
   ],
   "source": [
    "# We will now get all the data for each of the company irrespective of location\n",
    "fileCounter=0\n",
    "FullCompanyData=pd.DataFrame(columns=['property','hasValue','CompanyName'])\n",
    "for curCompanyName in companyListAllLocations['CompanyName'].values:\n",
    "    companyQuery='''SELECT ?property ?hasValue\n",
    "    WHERE {\n",
    "      { <%s> ?property ?hasValue }\n",
    "      UNION\n",
    "      { ?hasValue ?property <%s> }\n",
    "    }''' % (curCompanyName,curCompanyName)\n",
    "    curCompanyData=returnDF(get_dbpedia_sparql_data(companyQuery))\n",
    "    curCompanyData['Company']=curCompanyName\n",
    "    curCompanyData.columns=['property','hasValue','CompanyName']\n",
    "    FullCompanyData=pd.concat([FullCompanyData,curCompanyData])\n",
    "    if(FullCompanyData.shape[0] > 100000):    \n",
    "        FullCompanyData.to_csv('/home/ubuntu/anant/data/CompanyDataSansLocation_{}.txt'.format(fileCounter),index=False,encoding='utf-8')\n",
    "        fileCounter=fileCounter + 1\n",
    "        FullCompanyData=pd.DataFrame(columns=['property','hasValue','CompanyName'])\n",
    "        print(\"Written to file {}\".format(fileCounter-1))\n",
    "\n",
    "FullCompanyData.to_csv('/home/ubuntu/anant/data/CompanyDataSansLocation_{}.txt'.format(fileCounter),index=False,encoding='utf-8')\n",
    "fileCounter=fileCounter + 1\n",
    "FullCompanyData=pd.DataFrame(columns=['property','hasValue','CompanyName'])\n",
    "print(\"Written to file {}\".format(fileCounter-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the predicate data for all the RHS\n",
    "predicateList=pd.read_csv('/home/ubuntu/anant/data/predicateListJoins.txt',header=-1)\n",
    "predicateList.columns=['property']\n",
    "predicateList['property']=predicateList['property'].map(lambda x : str(x).replace('<','').replace('>',''))\n",
    "\n",
    "# Complete list of all predicates\n",
    "predicateValues=[]\n",
    "\n",
    "for curFileName in os.listdir('/home/ubuntu/anant/data/'):\n",
    "    if('CompanyDataSansLocation' in curFileName ):\n",
    "        FullCompanyData=pd.read_csv('/home/ubuntu/anant/data/{}'.format(curFileName))\n",
    "        tempValues=list(FullCompanyData.merge(predicateList,left_on='property',right_on='property',how='inner')['hasValue'].unique())\n",
    "        predicateValues=predicateValues + tempValues\n",
    "        \n",
    "predicateValues=list(set(predicateValues))\n",
    "\n",
    "predicateData=pd.DataFrame(columns=['property','hasValue','Predicate'])\n",
    "fileCounter=0\n",
    "\n",
    "for curPredicate in predicateValues:\n",
    "    predicateQuery='''SELECT ?property ?hasValue\n",
    "    WHERE {\n",
    "      { <%s> ?property ?hasValue }\n",
    "      UNION\n",
    "      { ?hasValue ?property <%s> }\n",
    "    }''' % (curPredicate,curPredicate)\n",
    "    try:\n",
    "        curpredicateData=returnDF(get_dbpedia_sparql_data(predicateQuery))\n",
    "        curpredicateData['Predicate']=curPredicate\n",
    "        curpredicateData.columns=['property','hasValue','Predicate']\n",
    "        predicateData=pd.concat([predicateData,curpredicateData])\n",
    "    except:\n",
    "        print(\"Error for {}\".format(curPredicate))\n",
    "    if(predicateData.shape[0] > 100000):    \n",
    "        predicateData.to_csv('/home/ubuntu/anant/data/PredicateDataSansLocation_{}.txt'.format(fileCounter),index=False,encoding='utf-8')\n",
    "        fileCounter=fileCounter + 1\n",
    "        print(\"Written to file {} with shape {}\".format(fileCounter-1,predicateData.shape[0]))\n",
    "        predicateData=pd.DataFrame(columns=['property','hasValue','Predicate'])\n",
    "\n",
    "predicateData.to_csv('/home/ubuntu/anant/data/PredicateDataSansLocation_{}.txt'.format(fileCounter),index=False,encoding='utf-8')\n",
    "fileCounter=fileCounter + 1\n",
    "print(\"Written to file {} with shape {}\".format(fileCounter-1,predicateData.shape[0]))\n",
    "predicateData=pd.DataFrame(columns=['property','hasValue','Predicate'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
