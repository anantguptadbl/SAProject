{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "fullData=[]\n",
    "for pageNumber in range(1,500):\n",
    "    print(\"Getting current page number {}\".format(pageNumber))\n",
    "    initData=requests.get('http://techspy.com/?tab=Home&page={}'.format(pageNumber)).content\n",
    "    initData=initData.replace('\\r','').replace('\\n','')\n",
    "    initsoup = BeautifulSoup(initData, 'html.parser')\n",
    "    # Getting the initial articles\n",
    "    articleLinks = initsoup.findAll(\"div\", {\"class\": \"si-content\"})\n",
    "    for curArticle in list(articleLinks):\n",
    "        innerArticle=list(curArticle.children)[0]['href']\n",
    "        # Getting the detailed articles\n",
    "        innerData=requests.get('http://www.techspy.com/' + innerArticle).content\n",
    "        innerData=innerData.replace('\\r','').replace('\\n','')\n",
    "        innersoup = BeautifulSoup(innerData, 'html.parser')\n",
    "        try:\n",
    "            title=innersoup.find('h1',{\"class\":\" sd-title\"}).text\n",
    "            article=innersoup.find('p',{\"class\":\" sd-text\"}).text\n",
    "            time=innersoup.find('span',{\"class\":\" sd-time\"}).text\n",
    "            fullData.append([title,article,time])\n",
    "        except:\n",
    "            print(\"Moving on\")\n",
    "    print(\"After pagenumber {} the length is {}\".format(pageNumber,len(fullData)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import os\n",
    "\n",
    "path_to_phantomJS='/home/ubuntu/anant/SAProject/apps/phantomjs'\n",
    "browser = webdriver.PhantomJS(executable_path = path_to_phantomJS,service_log_path=os.path.devnull)\n",
    "browser.get('http://techspy.com/?tab=Home&page=1')\n",
    "pageContents=browser.page_source\n",
    "browser.close()\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullData=[]\n",
    "initsoup = BeautifulSoup(pageContents, 'html.parser')\n",
    "# Getting the initial articles\n",
    "articleLinks = initsoup.findAll(\"div\", {\"class\": \"si-content\"})\n",
    "for curArticle in list(articleLinks):\n",
    "    innerArticle=list(curArticle.children)[0]['href']\n",
    "    # Getting the detailed articles\n",
    "    browser = webdriver.PhantomJS(executable_path = path_to_phantomJS,service_log_path=os.path.devnull)\n",
    "    browser.get('http://www.techspy.com/' + innerArticle)\n",
    "    print('http://www.techspy.com/' + innerArticle)\n",
    "    innerData=browser.page_source\n",
    "    browser.close()\n",
    "    browser.quit()\n",
    "    innerData=innerData.replace('\\r','').replace('\\n','')\n",
    "    innersoup = BeautifulSoup(innerData, 'html.parser')\n",
    "    try:\n",
    "        title=innersoup.find('h1',{\"class\":\" sd-title\"}).text\n",
    "        article=innersoup.find('p',{\"class\":\" sd-text\"}).text\n",
    "        time=innersoup.find('span',{\"class\":\" sd-time\"}).text\n",
    "        fullData.append([title,article,time])\n",
    "    except:\n",
    "        print(\"Moving on\")\n",
    "print(\"After pagenumber {} the length is {}\".format(pageNumber,len(fullData)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "chromeOptions = Options()\n",
    "chromeOptions.binary_location='/home/ubuntu/anant/SAProject/apps/chromedriver'\n",
    "chromeOptions.set_headless(True)\n",
    "chromeOptions.add_argument('--headless')\n",
    "chromeOptions.add_argument('--no-sandbox')\n",
    "chromeOptions.add_argument('--disable-dev-shm-usage')\n",
    "chromeOptions.add_argument(\"start-maximized\")\n",
    "chromeOptions.add_argument('--disable-gpu')\n",
    "chromeOptions.add_argument(\"disable-infobars\")\n",
    "chromeOptions.add_argument(\"--disable-extensions\")\n",
    "chromeOptions.experimental_options['useAutomationExtension']=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNOWTECHIE DATA\n",
    "#FullData=[]\n",
    "path_to_phantomJS='/home/ubuntu/anant/SAProject/apps/phantomjs'\n",
    "path_to_chromeJS='/home/ubuntu/anant/SAProject/apps/chromedriver'\n",
    "#browser = webdriver.PhantomJS(executable_path = path_to_phantomJS,service_log_path=os.path.devnull)\n",
    "browser = webdriver.Chrome(chrome_options=chromeOptions,executable_path = path_to_chromeJS)\n",
    "browser.get('https://knowtechie.com/category/news/')\n",
    "print('https://knowtechie.com/category/news/')\n",
    "clickElem=browser.find_element_by_class_name(\"mvp-inf-more-but\")\n",
    "print(clickElem)\n",
    "clickElem.click()\n",
    "clickElem=browser.find_element_by_class_name(\"mvp-inf-more-but\")\n",
    "print(clickElem)\n",
    "clickElem.click()\n",
    "#browser.find_element_by_class_name(\"mvp-inf-more-but\").click()\n",
    "innerData=browser.page_source\n",
    "browser.close()\n",
    "browser.quit()\n",
    "Data=innerData.replace('\\r','').replace('\\n','')\n",
    "Soup = BeautifulSoup(innerData, 'html.parser')\n",
    "getKnowTechieData(Soup,FullData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKnowTechieData(Soup,fullData):\n",
    "    masterElement=Soup.find('ul',{\"class\" : \"mvp-main-blog-wrap left relative infinite-content\"})\n",
    "    for x in masterElement.find_all('li'):\n",
    "        fullDataText=''\n",
    "        curText=x.find('h2').text\n",
    "        curInnerLink=x.find('h2').find('a')['href']\n",
    "        browser = webdriver.PhantomJS(executable_path = path_to_phantomJS,service_log_path=os.path.devnull)\n",
    "        browser.get(curInnerLink)\n",
    "        print(\"Getting data for {} \".format(curInnerLink))\n",
    "        innerData=browser.page_source\n",
    "        browser.close()\n",
    "        browser.quit()\n",
    "        innerData=innerData.replace('\\r','').replace('\\n','')\n",
    "        innerSoup = BeautifulSoup(innerData, 'html.parser')\n",
    "        innerTime=innerSoup.find('time',{\"class\":\"post-date updated\"}).text\n",
    "        dataDiv=innerSoup.find('div',{\"id\":\"mvp-content-main\"})\n",
    "        for dataElem in dataDiv.find_all('p'):\n",
    "            fullDataText= fullDataText +' ' + dataElem.text\n",
    "        fullData.append([curInnerLink,innerTime,fullDataText]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the ALL the News Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "FullData=pd.DataFrame(columns=['id','title','publication','author','date','year','month','url','content'])\n",
    "def getData(fileName):\n",
    "    data=[]\n",
    "    counter=1\n",
    "    with open('../data/kaggle/all-the-news/{}'.format(fileName),'r') as f1:\n",
    "        for line in f1:\n",
    "            if counter > 1:\n",
    "                curCounter=line[0:line.find(',')]\n",
    "                line=line[line.find(',')+1:]\n",
    "                curId=line[0:line.find(',')]\n",
    "                line=line[line.find(',')+1:]\n",
    "                line=line.strip()\n",
    "                if(len(str(line)) > 0):\n",
    "                    if(line[0]=='\"'):\n",
    "                        line=line[1:]\n",
    "                        curTitle=line[0:line.find('\"')]\n",
    "                        line=line[line.find('\"')+1:]\n",
    "                        line=line[line.find(',')+1:]\n",
    "                    else:\n",
    "                        # Normal Processing\n",
    "                        curTitle=line[0:line.find(',')]\n",
    "                        line=line[line.find(',')+1:]\n",
    "                    curPublication=line[0:line.find(',')]\n",
    "                    line=line[line.find(',')+1:]\n",
    "                    curAuthor=line[0:line.find(',')]\n",
    "                    line=line[line.find(',')+1:]\n",
    "                    curDate=line[0:line.find(',')]\n",
    "                    line=line[line.find(',')+1:]\n",
    "                    curYear=line[0:line.find(',')]\n",
    "                    line=line[line.find(',')+1:]\n",
    "                    curMonth=line[0:line.find(',')]\n",
    "                    line=line[line.find(',')+1:]\n",
    "                    curURL=line[0:line.find(',')]\n",
    "                    line=line[line.find(',')+2:]\n",
    "                    curContent=line\n",
    "                    data.append([curId,curTitle,curPublication,curAuthor,curDate,curYear,curMonth,curURL,curContent])\n",
    "            counter=counter+1\n",
    "    data=pd.DataFrame(data,columns=['id','title','publication','author','date','year','month','url','content'])\n",
    "    return(data)\n",
    "\n",
    "for curFileName in os.listdir('../data/kaggle/all-the-news/'):\n",
    "    print(curFileName)\n",
    "    FullData=pd.concat([FullData,getData(curFileName)])\n",
    "    print(FullData.shape)\n",
    "    \n",
    "FullData.to_csv('../data/allNewsFullData.txt',sep=chr(255),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the news data\n",
    "# GNF DATA\n",
    "#!head -5  /home/ubuntu/anant/SAProject/data/kaggle/GNF/file1aa\n",
    "GNFData=pd.DataFrame(columns=['publish_time','feed_code','source_url','headline_text'])\n",
    "for curFile in os.listdir('../data/kaggle/GNF/'):\n",
    "    print(curFile)\n",
    "    curData=pd.read_csv('../data/kaggle/GNF/{}'.format(curFile),skiprows=1,engine='python',sep=',',skip_blank_lines=True,error_bad_lines=False)\n",
    "    curData.columns=['publish_time','feed_code','source_url','headline_text']\n",
    "    for curcol in curData.columns:\n",
    "        curData[curcol]=curData[curcol].map(lambda x : ''.join([y for y in str(x) if ord(y) <= 126 and ord(y) >= 32]))\n",
    "    GNFData=pd.concat([GNFData,curData])\n",
    "    print(GNFData.shape)\n",
    "GNFData.to_csv('../data/GNFHeadLineData.txt',sep=chr(255),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Expansion Algorithm\n",
    "#############################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "companyData=pd.read_csv('../data/CompanyDataSansLocationFull.txt')\n",
    "#companyData=pd.read_csv('/home/anantgupta/Documents/Programming/Projects/situationalAwareness/SAProject/CompanyDataSansLocationFull.txt')\n",
    "\n",
    "products=companyData[companyData['propertyLabel']=='products']['valueLabel'].unique()\n",
    "companies=companyData['CompanyLabel'].unique()\n",
    "\n",
    "def processCompany(x):\n",
    "    # We need to first convert all the non alphanumeric to ''\n",
    "    x=str(x).replace('\"','')\n",
    "    x=re.sub(r'\\([^)]*\\)', '',x)\n",
    "    if(len(x.strip()) <= 3):\n",
    "        x=''\n",
    "    return(x.strip())\n",
    "\n",
    "companyData['companyLabelProcessed']=companyData['CompanyLabel'].map(lambda x : processCompany(x))\n",
    "companyData.to_csv('../data/CompanyDataSansLocationFull.txt',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "def getContinousBigrams(inpArray):\n",
    "    outArray=[]\n",
    "    for curElement in inpArray:\n",
    "        tempArray=curElement.split(' ')\n",
    "        if(len(tempArray) > 2):\n",
    "            tempArray\n",
    "        else:\n",
    "            outArray.append(curElement)\n",
    "\n",
    "def getDetailedProducts(x,allElements):\n",
    "    indItems=x.split(',')\n",
    "    if(len(indItems) > 1):\n",
    "        indItems=[ ' '.join([str(y).strip().lower() for y in curItem.split(' ') if str(y).lower() not in stopWords]).strip() for curItem in indItems]\n",
    "        # Now for all the individual items, we will prepare a list of words that we need to search based on importance\n",
    "        bigrams = [' '.join(b) for l in indItems for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
    "        allElements[x]=bigrams\n",
    "        #allElements.extend(indItems)\n",
    "    else:\n",
    "        indItems=' '.join([str(y).strip().lower() for y in indItems[0].split(' ') if str(y).lower() not in stopWords])\n",
    "        bigrams = [' '.join(b) for l in [indItems] for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
    "        allElements[x]=bigrams\n",
    "        #allElements.append(indItems.strip())\n",
    "        \n",
    "allElements={}\n",
    "[getDetailedProducts(x,allElements) for x in products]\n",
    "#allElements=list(set(allElements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some papers\n",
    "#https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=2ahUKEwiz7vmB0abeAhXLWysKHVepAB8QFjAAegQICRAB&url=https%3A%2F%2Fstackoverflow.com%2Fquestions%2F21844546%2Fforming-bigrams-of-words-in-list-of-sentences-with-python&usg=AOvVaw1cF0_ZkiXJy-HXku5ozxhd\n",
    "#Discovering expansion entities for keyword-based entity search in linked data\n",
    "\n",
    "# Our approach\n",
    "# We will find the most commoinly occurring grammar linkages between the entity and aspect\n",
    "\n",
    "def cleanNewsText(x):\n",
    "    if(x[0:4]=='http'):\n",
    "        return ''\n",
    "    else:\n",
    "        return(''.join([y for y in str(x) if ord(y) <= 126 and ord(y) >= 32]))\n",
    "\n",
    "FullData['contentNew']=FullData['content'].map(lambda x : cleanNewsText(x))\n",
    "\n",
    "# Saving the DataFrame\n",
    "FullData.reset_index(inplace=True)\n",
    "FullData['contentNew']=FullData[['contentNew']].fillna('').values\n",
    "FullData=FullData[FullData['contentNew'] != '']\n",
    "FullData.to_csv('../../SADataCompany.txt',index=False)\n",
    "\n",
    "# Also saving the company Dataframe\n",
    "pd.DataFrame(companies,columns=['companyName']).to_csv('../../companyNameList.txt',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anantgupta/.conda/envs/python2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2714: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "FullData=pd.read_csv('../../SADataCompany.txt')\n",
    "companyData=pd.read_csv('../data/CompanyDataSansLocationFull.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#products=companyData[companyData['propertyLabel']=='products']['valueLabel'].unique()\n",
    "products=[]\n",
    "def getProducts(x,products):\n",
    "    products.extend([y.strip().lower() for y in x.split(',')])\n",
    "\n",
    "[getProducts(x,products) for x in companyData[companyData['propertyLabel']=='products']['valueLabel'].unique()]\n",
    "products=list(set(products))\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def convertLemma(x):\n",
    "    return(' '.join([wordnet_lemmatizer.lemmatize(y) for y in x.split(' ')]))\n",
    "\n",
    "products=[convertLemma(x) for x in products]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Processing for the batch start index 0Started Processing for the batch start index 200\n",
      "Started Processing for the batch start index 100\n",
      "Started Processing for the batch start index 300Started Processing for the batch start index 400 Started Processing for the batch start index 600 Finished Processing for the batch start index 200\n",
      "Finished Processing for the batch start index 0\n",
      "\n",
      "Started Processing for the batch start index 700Started Processing for the batch start index 500\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 300Finished Processing for the batch start index 400\n",
      "\n",
      "Finished Processing for the batch start index 600\n",
      "Finished Processing for the batch start index 100Finished Processing for the batch start index 500\n",
      "\n",
      "Finished Processing for the batch start index 700\n",
      "\n",
      "\n",
      "Started Processing for the batch start index 800 \n",
      "Finished Processing for the batch start index 800 Started Processing for the batch start index 900Started Processing for the batch start index 1100  Started Processing for the batch start index 1400Started Processing for the batch start index 1500 \n",
      "\n",
      "\n",
      "\n",
      "Started Processing for the batch start index 1200Started Processing for the batch start index 1300\n",
      "Finished Processing for the batch start index 1400Started Processing for the batch start index 1000Finished Processing for the batch start index 900\n",
      "Finished Processing for the batch start index 1100\n",
      "\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 1300\n",
      "Finished Processing for the batch start index 1200\n",
      "Finished Processing for the batch start index 1000\n",
      "\n",
      "Started Processing for the batch start index 1600\n",
      "Finished Processing for the batch start index 1500 \n",
      "Started Processing for the batch start index 1700 Finished Processing for the batch start index 1600\n",
      "Started Processing for the batch start index 1800\n",
      " Finished Processing for the batch start index 1700\n",
      "Completed for the following batch of 50 threads 0 in 1753.76663303 secondsStarted Processing for the batch start index 1900\n",
      "\n",
      "Finished Processing for the batch start index 1800\n",
      "Started Processing for the batch start index 2000Started Processing for the batch start index 2100 Finished Processing for the batch start index 1900Started Processing for the batch start index 2300\n",
      "Started Processing for the batch start index 2400\n",
      "\n",
      "Started Processing for the batch start index 2200\n",
      "Started Processing for the batch start index 2700\n",
      "Finished Processing for the batch start index 2000Finished Processing for the batch start index 2100\n",
      "Started Processing for the batch start index 2500Started Processing for the batch start index 2600 Started Processing for the batch start index 2900 Started Processing for the batch start index 3100Started Processing for the batch start index 3200  Started Processing for the batch start index 3500 Started Processing for the batch start index 3700\n",
      "Finished Processing for the batch start index 2300\n",
      "Finished Processing for the batch start index 2400\n",
      "\n",
      "Finished Processing for the batch start index 2200\n",
      "\n",
      "Started Processing for the batch start index 3900Started Processing for the batch start index 2800\n",
      "Started Processing for the batch start index 3000\n",
      "\n",
      "Started Processing for the batch start index 3300Started Processing for the batch start index 3400\n",
      "Started Processing for the batch start index 3600\n",
      "Started Processing for the batch start index 3800\n",
      "\n",
      "Finished Processing for the batch start index 2700\n",
      "Finished Processing for the batch start index 3700\n",
      "\n",
      "Finished Processing for the batch start index 2500Finished Processing for the batch start index 2600\n",
      "\n",
      "Finished Processing for the batch start index 2900\n",
      "Finished Processing for the batch start index 3100Finished Processing for the batch start index 3200\n",
      "\n",
      " Finished Processing for the batch start index 3300\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 3000Finished Processing for the batch start index 2800\n",
      "Finished Processing for the batch start index 3400\n",
      "\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 3500 \n",
      "Finished Processing for the batch start index 3800Finished Processing for the batch start index 3900Finished Processing for the batch start index 3600Completed for the following batch of 50 threads 20 in 2259.69875002 seconds\n",
      "\n",
      "\n",
      "Started Processing for the batch start index 4000\n",
      "\n",
      " Started Processing for the batch start index 4200Started Processing for the batch start index 4300\n",
      "Finished Processing for the batch start index 4000Started Processing for the batch start index 4100\n",
      "Finished Processing for the batch start index 4200\n",
      "\n",
      "Started Processing for the batch start index 4400 Finished Processing for the batch start index 4300\n",
      "Finished Processing for the batch start index 4100\n",
      "Started Processing for the batch start index 4500  Started Processing for the batch start index 4800   Started Processing for the batch start index 5200\n",
      "  \n",
      "Finished Processing for the batch start index 4400\n",
      "Started Processing for the batch start index 4600Started Processing for the batch start index 4700\n",
      "Started Processing for the batch start index 5500Started Processing for the batch start index 4900Started Processing for the batch start index 5000Started Processing for the batch start index 5100\n",
      "Started Processing for the batch start index 5600Started Processing for the batch start index 5700 Started Processing for the batch start index 5300Started Processing for the batch start index 5400\n",
      "\n",
      "Finished Processing for the batch start index 4500\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 5200\n",
      "Started Processing for the batch start index 5900Started Processing for the batch start index 5800\n",
      "\n",
      "Finished Processing for the batch start index 5700Completed for the following batch of 50 threads 40 in 3303.17686987 seconds\n",
      "Finished Processing for the batch start index 4600Finished Processing for the batch start index 4700Finished Processing for the batch start index 4800Finished Processing for the batch start index 5500Finished Processing for the batch start index 4900Finished Processing for the batch start index 5000Finished Processing for the batch start index 5100\n",
      "Finished Processing for the batch start index 5600\n",
      "Started Processing for the batch start index 6000Started Processing for the batch start index 6100 Started Processing for the batch start index 6300Started Processing for the batch start index 6400Started Processing for the batch start index 6500Started Processing for the batch start index 6600 Started Processing for the batch start index 6800Started Processing for the batch start index 6900 \n",
      "Finished Processing for the batch start index 5300Finished Processing for the batch start index 5400\n",
      "\n",
      "Started Processing for the batch start index 7300\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "Finished Processing for the batch start index 5900\n",
      "\n",
      "Started Processing for the batch start index 6200\n",
      "\n",
      "\n",
      "\n",
      "Started Processing for the batch start index 6700\n",
      "\n",
      "Started Processing for the batch start index 7000Started Processing for the batch start index 7100Started Processing for the batch start index 7200 Started Processing for the batch start index 7500Started Processing for the batch start index 7700Started Processing for the batch start index 7800Started Processing for the batch start index 7900Started Processing for the batch start index 8000 Started Processing for the batch start index 8200Finished Processing for the batch start index 5800\n",
      "\n",
      " Completed for the following batch of 50 threads 60 in 4058.82988405 seconds\n",
      "\n",
      " \n",
      " Started Processing for the batch start index 8100\n",
      "Started Processing for the batch start index 8600Started Processing for the batch start index 7600Started Processing for the batch start index 8700\n",
      "Finished Processing for the batch start index 6000 \n",
      "Finished Processing for the batch start index 6300Finished Processing for the batch start index 6400Finished Processing for the batch start index 6500Finished Processing for the batch start index 6600\n",
      "Finished Processing for the batch start index 6800Finished Processing for the batch start index 6900\n",
      "\n",
      "\n",
      "Started Processing for the batch start index 7400 \n",
      "\n",
      "\n",
      "\n",
      " Started Processing for the batch start index 8900 Started Processing for the batch start index 9100   \n",
      " Started Processing for the batch start index 9600Started Processing for the batch start index 8400\n",
      "Finished Processing for the batch start index 7300Started Processing for the batch start index 9500Started Processing for the batch start index 8300Finished Processing for the batch start index 8200Started Processing for the batch start index 8500\n",
      "Finished Processing for the batch start index 8000\n",
      "\n",
      "\n",
      "Started Processing for the batch start index 9900\n",
      "Finished Processing for the batch start index 6100Finished Processing for the batch start index 6200\n",
      "\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 6700\n",
      "\n",
      "Finished Processing for the batch start index 7000Finished Processing for the batch start index 7100Finished Processing for the batch start index 7200\n",
      "Started Processing for the batch start index 9200Finished Processing for the batch start index 7500Finished Processing for the batch start index 7700Finished Processing for the batch start index 7800Finished Processing for the batch start index 7900Started Processing for the batch start index 8800\n",
      "Started Processing for the batch start index 9000\n",
      "Started Processing for the batch start index 9300Started Processing for the batch start index 9400 Started Processing for the batch start index 10000 Started Processing for the batch start index 10200    Started Processing for the batch start index 10700\n",
      "Started Processing for the batch start index 9700\n",
      "\n",
      "Completed for the following batch of 50 threads 80 in 2859.94633293 seconds\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 8100\n",
      "Finished Processing for the batch start index 8600Finished Processing for the batch start index 7600Finished Processing for the batch start index 8700\n",
      " \n",
      "\n",
      "  Started Processing for the batch start index 11600Started Processing for the batch start index 11200\n",
      "Started Processing for the batch start index 11800Started Processing for the batch start index 11300\n",
      "\n",
      "Finished Processing for the batch start index 7400\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 8900\n",
      "Finished Processing for the batch start index 9100\n",
      "\n",
      "Started Processing for the batch start index 9800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Processing for the batch start index 10100\n",
      "Started Processing for the batch start index 10300Started Processing for the batch start index 10400Started Processing for the batch start index 10500Started Processing for the batch start index 10600  Started Processing for the batch start index 11000Started Processing for the batch start index 11100Started Processing for the batch start index 11900Started Processing for the batch start index 12000  Started Processing for the batch start index 12300Started Processing for the batch start index 12400 Started Processing for the batch start index 12600  Started Processing for the batch start index 12900Finished Processing for the batch start index 10700\n",
      "Finished Processing for the batch start index 9600Finished Processing for the batch start index 8400\n",
      "Started Processing for the batch start index 13200Finished Processing for the batch start index 9500Finished Processing for the batch start index 8300 Finished Processing for the batch start index 8500\n",
      "Started Processing for the batch start index 13500\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 9900Started Processing for the batch start index 11400Started Processing for the batch start index 13600 Started Processing for the batch start index 11500Started Processing for the batch start index 11700\n",
      "\n",
      "Started Processing for the batch start index 13800\n",
      "\n",
      "\n",
      "  \n",
      "Finished Processing for the batch start index 9200Started Processing for the batch start index 13000Started Processing for the batch start index 13900\n",
      "Started Processing for the batch start index 12800Finished Processing for the batch start index 8800\n",
      "Finished Processing for the batch start index 9000\n",
      "Finished Processing for the batch start index 9300Finished Processing for the batch start index 9400\n",
      "Finished Processing for the batch start index 10000\n",
      "Finished Processing for the batch start index 10200\n",
      "\n",
      "\n",
      "\n",
      "Started Processing for the batch start index 10800Started Processing for the batch start index 10900\n",
      "\n",
      "\n",
      "\n",
      "Started Processing for the batch start index 12100Started Processing for the batch start index 14200Started Processing for the batch start index 12200\n",
      "\n",
      "Started Processing for the batch start index 12500\n",
      "Started Processing for the batch start index 12700Started Processing for the batch start index 14000Started Processing for the batch start index 14100\n",
      "Finished Processing for the batch start index 9700\n",
      "\n",
      "Completed for the following batch of 50 threads 100 in 2091.20131207 seconds\n",
      "\n",
      "\n",
      "Started Processing for the batch start index 13400\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 12600\n",
      "\n",
      "\n",
      "Started Processing for the batch start index 13300\n",
      "\n",
      "Finished Processing for the batch start index 11600Finished Processing for the batch start index 11200\n",
      "Finished Processing for the batch start index 11800Finished Processing for the batch start index 11300\n",
      "Started Processing for the batch start index 13700Started Processing for the batch start index 13100Finished Processing for the batch start index 12400\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 12900\n",
      "\n",
      "Finished Processing for the batch start index 12300\n",
      "\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 9800\n",
      "Finished Processing for the batch start index 10100\n",
      "Finished Processing for the batch start index 10300Finished Processing for the batch start index 10400Finished Processing for the batch start index 10500Finished Processing for the batch start index 10600\n",
      "\n",
      " Finished Processing for the batch start index 11100Finished Processing for the batch start index 11900Finished Processing for the batch start index 12000\n",
      "\n",
      "Finished Processing for the batch start index 14200\n",
      "Finished Processing for the batch start index 12100\n",
      "Finished Processing for the batch start index 13200\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 11000Finished Processing for the batch start index 14100Finished Processing for the batch start index 13500Finished Processing for the batch start index 14000Finished Processing for the batch start index 12700\n",
      "Finished Processing for the batch start index 10900Finished Processing for the batch start index 11400Finished Processing for the batch start index 13600\n",
      "Finished Processing for the batch start index 11500Finished Processing for the batch start index 11700\n",
      "\n",
      "Finished Processing for the batch start index 13800\n",
      "\n",
      "Finished Processing for the batch start index 12500\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 10800Finished Processing for the batch start index 13000Finished Processing for the batch start index 13900\n",
      "Finished Processing for the batch start index 12800\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 12200\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Completed for the following batch of 50 threads 120 in 870.789866924 seconds\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 13400\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 13300\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 13100\n",
      "Finished Processing for the batch start index 13700\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Completed for the following batch of 50 threads 140 in 214.210039854 seconds\n",
      "Completed for the following batch of 50 threads 160 in 2.14576721191e-06 seconds\n",
      "Completed for the following batch of 50 threads 180 in 2.14576721191e-06 seconds\n",
      "Completed for the following batch of 50 threads 200 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 220 in 2.14576721191e-06 seconds\n",
      "Completed for the following batch of 50 threads 240 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 260 in 0.0 seconds\n",
      "Completed for the following batch of 50 threads 280 in 2.14576721191e-06 seconds\n",
      "Completed for the following batch of 50 threads 300 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 320 in 1.19209289551e-06 seconds\n",
      "Completed for the following batch of 50 threads 340 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 360 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 380 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 400 in 0.0 seconds\n",
      "Completed for the following batch of 50 threads 420 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 440 in 2.14576721191e-06 seconds\n",
      "Completed for the following batch of 50 threads 460 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 480 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 500 in 0.0 seconds\n",
      "Completed for the following batch of 50 threads 520 in 0.0 seconds\n",
      "Completed for the following batch of 50 threads 540 in 2.14576721191e-06 seconds\n",
      "Completed for the following batch of 50 threads 560 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 580 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 600 in 1.90734863281e-06 seconds\n",
      "Completed for the following batch of 50 threads 620 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 640 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 660 in 0.0 seconds\n",
      "Completed for the following batch of 50 threads 680 in 1.19209289551e-06 seconds\n",
      "Completed for the following batch of 50 threads 700 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 720 in 1.19209289551e-06 seconds\n",
      "Completed for the following batch of 50 threads 740 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 760 in 0.0 seconds\n",
      "Completed for the following batch of 50 threads 780 in 2.14576721191e-06 seconds\n",
      "Completed for the following batch of 50 threads 800 in 1.19209289551e-06 seconds\n",
      "Completed for the following batch of 50 threads 820 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 840 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 860 in 1.19209289551e-06 seconds\n",
      "Completed for the following batch of 50 threads 880 in 1.19209289551e-06 seconds\n",
      "Completed for the following batch of 50 threads 900 in 1.19209289551e-06 seconds\n",
      "Completed for the following batch of 50 threads 920 in 1.19209289551e-06 seconds\n",
      "Completed for the following batch of 50 threads 940 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 960 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 980 in 2.14576721191e-06 seconds\n",
      "Completed for the following batch of 50 threads 1000 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 1020 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 1040 in 0.0 seconds\n",
      "Completed for the following batch of 50 threads 1060 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 1080 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 1100 in 0.0 seconds\n",
      "Completed for the following batch of 50 threads 1120 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 1140 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 1160 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 1180 in 1.90734863281e-06 seconds\n",
      "Completed for the following batch of 50 threads 1200 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 1220 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 1240 in 0.0 seconds\n",
      "Completed for the following batch of 50 threads 1260 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 1280 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 1300 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 1320 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 1340 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 1360 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 1380 in 9.53674316406e-07 seconds\n",
      "Completed for the following batch of 50 threads 1400 in 0.0 seconds\n",
      "Completed for the following batch of 50 threads 1420 in 9.53674316406e-07 seconds\n"
     ]
    }
   ],
   "source": [
    "#FullData.head(5)\n",
    "#np.range(FullData.shape[0],1000)\n",
    "#np.arange(1,FullData.shape[0],1000)\n",
    "#df1, df2 = np.split(df, [72], axis=1)\n",
    "\n",
    "import time\n",
    "from threading import Thread\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "companies=companyData['companyLabelProcessed'].values\n",
    "companies=[str(x) for x in companies]\n",
    "\n",
    "# This method is very naive, as it does not make the split checks, but for now we will go ahead with this only\n",
    "# This clearly requires refinement\n",
    "results=[]\n",
    "def findCompanies(x):\n",
    "    if len(x) > 10:\n",
    "        matches=list(set([y for y in companies if y in x]))\n",
    "        return([str(curMatch) for curMatch in matches if len(str(curMatch)) > 1])\n",
    "    else:\n",
    "        return([''])\n",
    "\n",
    "def getNewsBatch(batchIndex,curList,results):\n",
    "    print \"Started Processing for the batch start index {}\".format(batchIndex)\n",
    "    results.append([[x[0],findCompanies(str(x[1]))] for x in curList])\n",
    "    print \"Finished Processing for the batch start index {}\".format(batchIndex)\n",
    "\n",
    "def calculateParallel(numbers, threads=2):\n",
    "    pool = ThreadPool(100)\n",
    "    results = pool.map(getNewsBatch, numbers)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return results\n",
    "    \n",
    "threadList=[]\n",
    "#for i in range(0,143000,100):\n",
    "#for i in range(14300,143000,100):\n",
    "for i in range(0,14300,100):\n",
    "    batchIndex=i\n",
    "    curList=FullData[i:i+100][['index','contentNew']].values\n",
    "    threadList.append(Thread(target=getNewsBatch, args=(batchIndex,curList,results,)))\n",
    "\n",
    "#for curIter in range(0,1430-143,20):\n",
    "for curIter in range(0,1430,20):\n",
    "    startTime=time.time()\n",
    "    curThreadList=threadList[curIter:curIter + 20]\n",
    "    for curThread in curThreadList:\n",
    "        curThread.start()\n",
    "    for curThread in curThreadList:\n",
    "        curThread.join()\n",
    "    print(\"Completed for the following batch of 50 threads {} in {} seconds\".format(curIter,str(time.time() - startTime)))\n",
    "    \n",
    "resultsPD=[]\n",
    "for x in results:\n",
    "    for y in x:\n",
    "        resultsPD.append(y)\n",
    "resultsPD=pd.DataFrame(resultsPD,columns=['index','companyMatch'])\n",
    "resultsPD['lengthVal']=resultsPD['companyMatch'].map(lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsPD.to_csv('../../companyMatches.txt',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 2468, 2469, 2470])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FullData['index']=FullData['index'].values.astype(int)\n",
    "#FullData['index'].values\n",
    "#FullData[['index','contentNew']].merge(resultsPD[['index','companyMatch']],left_on='index',right_on='index',how='inner').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsPD=resultsPD.drop_duplicates()\n",
    "FullData['index']=FullData['index'].values.astype(int)\n",
    "\n",
    "# Merging the data\n",
    "workingData=FullData.merge(resultsPD,left_on='index',right_on='index',how='inner')\n",
    "\n",
    "# Writing the data\n",
    "workingData.to_csv('../../workingData.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'products' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-284a9be1da1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# PRODUCTS MATCHING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mproducts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproducts\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# This method is very naive, as it does not make the split checks, but for now we will go ahead with this only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'products' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from threading import Thread\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "# PRODUCTS MATCHING\n",
    "products=[str(x) for x in products if len(str(x)) > 3]\n",
    "\n",
    "# This method is very naive, as it does not make the split checks, but for now we will go ahead with this only\n",
    "# This clearly requires refinement\n",
    "results=[]\n",
    "def findProducts(x):\n",
    "    if len(x) > 10:\n",
    "        matches=list(set([y for y in products if y in x]))\n",
    "        return([str(curMatch) for curMatch in matches if len(str(curMatch)) > 1])\n",
    "    else:\n",
    "        return([''])\n",
    "\n",
    "def getNewsBatch(batchIndex,curList,results):\n",
    "    print \"Started Processing for the batch start index {}\".format(batchIndex)\n",
    "    results.append([[x[0],findProducts(str(x[1]))] for x in curList])\n",
    "    print \"Finished Processing for the batch start index {}\".format(batchIndex)\n",
    "\n",
    "def calculateParallel(numbers, threads=2):\n",
    "    pool = ThreadPool(100)\n",
    "    results = pool.map(getNewsBatch, numbers)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return results\n",
    "    \n",
    "threadList=[]\n",
    "#for i in range(0,143000,100):\n",
    "#for i in range(14300,143000,100):\n",
    "for i in range(0,14300,100):\n",
    "    batchIndex=i\n",
    "    curList=FullData[i:i+100][['index','contentNew']].values\n",
    "    threadList.append(Thread(target=getNewsBatch, args=(batchIndex,curList,results,)))\n",
    "\n",
    "#for curIter in range(0,1430-143,20):\n",
    "for curIter in range(0,1430,20):\n",
    "    startTime=time.time()\n",
    "    curThreadList=threadList[curIter:curIter + 20]\n",
    "    for curThread in curThreadList:\n",
    "        curThread.start()\n",
    "    for curThread in curThreadList:\n",
    "        curThread.join()\n",
    "    print(\"Completed for the following batch of 50 threads {} in {} seconds\".format(curIter,str(time.time() - startTime)))\n",
    "\n",
    "resultsPD1=[]\n",
    "for x in results:\n",
    "    for y in x:\n",
    "        resultsPD1.append(y)\n",
    "resultsPD1=pd.DataFrame(resultsPD1,columns=['index','productMatch'])\n",
    "resultsPD1=resultsPD1.drop_duplicates()\n",
    "resultsPD1.to_csv('../../productMatches.txt',index=False)\n",
    "# Read back the data with the company Filter\n",
    "#workingData=pd.read_csv('../../workingData.csv')\n",
    "\n",
    "# Merging the data with the product match\n",
    "#workingData=workingData.merge(resultsPD,left_on='index',right_on='index',how='inner')\n",
    "\n",
    "# Writing the data back \n",
    "#workingData.to_csv('../../workingDataWithProducts.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsPD=[]\n",
    "for x in results:\n",
    "    for y in x:\n",
    "        resultsPD.append(y)\n",
    "resultsPD=pd.DataFrame(resultsPD,columns=['index','productMatch'])\n",
    "workingData=pd.read_csv('../../workingData.csv')\n",
    "\n",
    "# Merging the data\n",
    "workingData=workingData.merge(resultsPD,left_on='index',right_on='index',how='inner')\n",
    "\n",
    "# Writing the data\n",
    "workingData.to_csv('../../workingDataWithProducts.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
