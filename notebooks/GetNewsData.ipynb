{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "fullData=[]\n",
    "for pageNumber in range(1,500):\n",
    "    print(\"Getting current page number {}\".format(pageNumber))\n",
    "    initData=requests.get('http://techspy.com/?tab=Home&page={}'.format(pageNumber)).content\n",
    "    initData=initData.replace('\\r','').replace('\\n','')\n",
    "    initsoup = BeautifulSoup(initData, 'html.parser')\n",
    "    # Getting the initial articles\n",
    "    articleLinks = initsoup.findAll(\"div\", {\"class\": \"si-content\"})\n",
    "    for curArticle in list(articleLinks):\n",
    "        innerArticle=list(curArticle.children)[0]['href']\n",
    "        # Getting the detailed articles\n",
    "        innerData=requests.get('http://www.techspy.com/' + innerArticle).content\n",
    "        innerData=innerData.replace('\\r','').replace('\\n','')\n",
    "        innersoup = BeautifulSoup(innerData, 'html.parser')\n",
    "        try:\n",
    "            title=innersoup.find('h1',{\"class\":\" sd-title\"}).text\n",
    "            article=innersoup.find('p',{\"class\":\" sd-text\"}).text\n",
    "            time=innersoup.find('span',{\"class\":\" sd-time\"}).text\n",
    "            fullData.append([title,article,time])\n",
    "        except:\n",
    "            print(\"Moving on\")\n",
    "    print(\"After pagenumber {} the length is {}\".format(pageNumber,len(fullData)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import os\n",
    "\n",
    "path_to_phantomJS='/home/ubuntu/anant/SAProject/apps/phantomjs'\n",
    "browser = webdriver.PhantomJS(executable_path = path_to_phantomJS,service_log_path=os.path.devnull)\n",
    "browser.get('http://techspy.com/?tab=Home&page=1')\n",
    "pageContents=browser.page_source\n",
    "browser.close()\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullData=[]\n",
    "initsoup = BeautifulSoup(pageContents, 'html.parser')\n",
    "# Getting the initial articles\n",
    "articleLinks = initsoup.findAll(\"div\", {\"class\": \"si-content\"})\n",
    "for curArticle in list(articleLinks):\n",
    "    innerArticle=list(curArticle.children)[0]['href']\n",
    "    # Getting the detailed articles\n",
    "    browser = webdriver.PhantomJS(executable_path = path_to_phantomJS,service_log_path=os.path.devnull)\n",
    "    browser.get('http://www.techspy.com/' + innerArticle)\n",
    "    print('http://www.techspy.com/' + innerArticle)\n",
    "    innerData=browser.page_source\n",
    "    browser.close()\n",
    "    browser.quit()\n",
    "    innerData=innerData.replace('\\r','').replace('\\n','')\n",
    "    innersoup = BeautifulSoup(innerData, 'html.parser')\n",
    "    try:\n",
    "        title=innersoup.find('h1',{\"class\":\" sd-title\"}).text\n",
    "        article=innersoup.find('p',{\"class\":\" sd-text\"}).text\n",
    "        time=innersoup.find('span',{\"class\":\" sd-time\"}).text\n",
    "        fullData.append([title,article,time])\n",
    "    except:\n",
    "        print(\"Moving on\")\n",
    "print(\"After pagenumber {} the length is {}\".format(pageNumber,len(fullData)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "chromeOptions = Options()\n",
    "chromeOptions.binary_location='/home/ubuntu/anant/SAProject/apps/chromedriver'\n",
    "chromeOptions.set_headless(True)\n",
    "chromeOptions.add_argument('--headless')\n",
    "chromeOptions.add_argument('--no-sandbox')\n",
    "chromeOptions.add_argument('--disable-dev-shm-usage')\n",
    "chromeOptions.add_argument(\"start-maximized\")\n",
    "chromeOptions.add_argument('--disable-gpu')\n",
    "chromeOptions.add_argument(\"disable-infobars\")\n",
    "chromeOptions.add_argument(\"--disable-extensions\")\n",
    "chromeOptions.experimental_options['useAutomationExtension']=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNOWTECHIE DATA\n",
    "#FullData=[]\n",
    "path_to_phantomJS='/home/ubuntu/anant/SAProject/apps/phantomjs'\n",
    "path_to_chromeJS='/home/ubuntu/anant/SAProject/apps/chromedriver'\n",
    "#browser = webdriver.PhantomJS(executable_path = path_to_phantomJS,service_log_path=os.path.devnull)\n",
    "browser = webdriver.Chrome(chrome_options=chromeOptions,executable_path = path_to_chromeJS)\n",
    "browser.get('https://knowtechie.com/category/news/')\n",
    "print('https://knowtechie.com/category/news/')\n",
    "clickElem=browser.find_element_by_class_name(\"mvp-inf-more-but\")\n",
    "print(clickElem)\n",
    "clickElem.click()\n",
    "clickElem=browser.find_element_by_class_name(\"mvp-inf-more-but\")\n",
    "print(clickElem)\n",
    "clickElem.click()\n",
    "#browser.find_element_by_class_name(\"mvp-inf-more-but\").click()\n",
    "innerData=browser.page_source\n",
    "browser.close()\n",
    "browser.quit()\n",
    "Data=innerData.replace('\\r','').replace('\\n','')\n",
    "Soup = BeautifulSoup(innerData, 'html.parser')\n",
    "getKnowTechieData(Soup,FullData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKnowTechieData(Soup,fullData):\n",
    "    masterElement=Soup.find('ul',{\"class\" : \"mvp-main-blog-wrap left relative infinite-content\"})\n",
    "    for x in masterElement.find_all('li'):\n",
    "        fullDataText=''\n",
    "        curText=x.find('h2').text\n",
    "        curInnerLink=x.find('h2').find('a')['href']\n",
    "        browser = webdriver.PhantomJS(executable_path = path_to_phantomJS,service_log_path=os.path.devnull)\n",
    "        browser.get(curInnerLink)\n",
    "        print(\"Getting data for {} \".format(curInnerLink))\n",
    "        innerData=browser.page_source\n",
    "        browser.close()\n",
    "        browser.quit()\n",
    "        innerData=innerData.replace('\\r','').replace('\\n','')\n",
    "        innerSoup = BeautifulSoup(innerData, 'html.parser')\n",
    "        innerTime=innerSoup.find('time',{\"class\":\"post-date updated\"}).text\n",
    "        dataDiv=innerSoup.find('div',{\"id\":\"mvp-content-main\"})\n",
    "        for dataElem in dataDiv.find_all('p'):\n",
    "            fullDataText= fullDataText +' ' + dataElem.text\n",
    "        fullData.append([curInnerLink,innerTime,fullDataText]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the ALL the News Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "FullData=pd.DataFrame(columns=['id','title','publication','author','date','year','month','url','content'])\n",
    "def getData(fileName):\n",
    "    data=[]\n",
    "    counter=1\n",
    "    with open('../data/kaggle/all-the-news/{}'.format(fileName),'r') as f1:\n",
    "        for line in f1:\n",
    "            if counter > 1:\n",
    "                curCounter=line[0:line.find(',')]\n",
    "                line=line[line.find(',')+1:]\n",
    "                curId=line[0:line.find(',')]\n",
    "                line=line[line.find(',')+1:]\n",
    "                line=line.strip()\n",
    "                if(len(str(line)) > 0):\n",
    "                    if(line[0]=='\"'):\n",
    "                        line=line[1:]\n",
    "                        curTitle=line[0:line.find('\"')]\n",
    "                        line=line[line.find('\"')+1:]\n",
    "                        line=line[line.find(',')+1:]\n",
    "                    else:\n",
    "                        # Normal Processing\n",
    "                        curTitle=line[0:line.find(',')]\n",
    "                        line=line[line.find(',')+1:]\n",
    "                if(len(str(line)) > 0):\n",
    "                    if(line[0]=='\"'):\n",
    "                        line=line[1:]\n",
    "                        curPublication=line[0:line.find('\"')]\n",
    "                        line=line[line.find('\"')+1:]\n",
    "                        line=line[line.find(',')+1:]\n",
    "                    else:\n",
    "                        # Normal Processing\n",
    "                        curPublication=line[0:line.find(',')]\n",
    "                        line=line[line.find(',')+1:]\n",
    "                if(len(str(line)) > 0):\n",
    "                    if(line[0]=='\"'):\n",
    "                        line=line[1:]\n",
    "                        curAuthor=line[0:line.find('\"')]\n",
    "                        line=line[line.find('\"')+1:]\n",
    "                        line=line[line.find(',')+1:]\n",
    "                    else:\n",
    "                        # Normal Processing\n",
    "                        curAuthor=line[0:line.find(',')]\n",
    "                        line=line[line.find(',')+1:]\n",
    "                if(len(str(line)) > 0):\n",
    "                    if(line[0]=='\"'):\n",
    "                        line=line[1:]\n",
    "                        curDate=line[0:line.find('\"')]\n",
    "                        line=line[line.find('\"')+1:]\n",
    "                        line=line[line.find(',')+1:]\n",
    "                    else:\n",
    "                        # Normal Processing\n",
    "                        curDate=line[0:line.find(',')]\n",
    "                        line=line[line.find(',')+1:]\n",
    "                if(len(str(line)) > 0):\n",
    "                    if(line[0]=='\"'):\n",
    "                        line=line[1:]\n",
    "                        curYear=line[0:line.find('\"')]\n",
    "                        line=line[line.find('\"')+1:]\n",
    "                        line=line[line.find(',')+1:]\n",
    "                    else:\n",
    "                        # Normal Processing\n",
    "                        curYear=line[0:line.find(',')]\n",
    "                        line=line[line.find(',')+1:]\n",
    "                if(len(str(line)) > 0):\n",
    "                    if(line[0]=='\"'):\n",
    "                        line=line[1:]\n",
    "                        curMonth=line[0:line.find('\"')]\n",
    "                        line=line[line.find('\"')+1:]\n",
    "                        line=line[line.find(',')+1:]\n",
    "                    else:\n",
    "                        # Normal Processing\n",
    "                        curMonth=line[0:line.find(',')]\n",
    "                        line=line[line.find(',')+1:]\n",
    "                if(len(str(line)) > 0):\n",
    "                    if(line[0]=='\"'):\n",
    "                        line=line[1:]\n",
    "                        curURL=line[0:line.find('\"')]\n",
    "                        line=line[line.find('\"')+1:]\n",
    "                        line=line[line.find(',')+1:]\n",
    "                    else:\n",
    "                        # Normal Processing\n",
    "                        curURL=line[0:line.find(',')]\n",
    "                        line=line[line.find(',')+1:]\n",
    "                #curPublication=line[0:line.find(',')]\n",
    "                #line=line[line.find(',')+1:]\n",
    "                #curAuthor=line[0:line.find(',')]\n",
    "                #line=line[line.find(',')+1:]\n",
    "                #curDate=line[0:line.find(',')]\n",
    "                #line=line[line.find(',')+1:]\n",
    "                #curYear=line[0:line.find(',')]\n",
    "                #line=line[line.find(',')+1:]\n",
    "                #curMonth=line[0:line.find(',')]\n",
    "                #line=line[line.find(',')+1:]\n",
    "                #curURL=line[0:line.find(',')]\n",
    "                line=line[line.find(',')+2:]\n",
    "                curContent=line\n",
    "                data.append([curId,curTitle,curPublication,curAuthor,curDate,curYear,curMonth,curURL,curContent])\n",
    "            counter=counter+1\n",
    "    data=pd.DataFrame(data,columns=['id','title','publication','author','date','year','month','url','content'])\n",
    "    return(data)\n",
    "\n",
    "for curFileName in os.listdir('../data/kaggle/all-the-news/'):\n",
    "    print(curFileName)\n",
    "    #FullData=pd.concat([FullData,getData(curFileName)])\n",
    "    FullData=pd.concat([FullData,pd.read_csv('../data/kaggle/all-the-news/{}'.format(curFileName),sep=',',error_bad_lines=False)])\n",
    "    print(FullData.shape)\n",
    "    \n",
    "#FullData.to_csv('../data/allNewsFullData.txt',sep=chr(255),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file1ac.bz2\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "',' expected after '\"'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-28f7fb45c525>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcurFile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/kaggle/GNF/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcurData\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/kaggle/GNF/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_blank_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merror_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m#curData=pd.read_csv('../data/kaggle/GNF/{}'.format(curFile),skiprows=1,error_bad_lines=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mcurData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'publish_time'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'feed_code'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'source_url'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'headline_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anantgupta/.conda/envs/python2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anantgupta/.conda/envs/python2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anantgupta/.conda/envs/python2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anantgupta/.conda/envs/python2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                                  ' \"c\", \"python\", or' ' \"python-fwf\")'.format(\n\u001b[1;32m   1023\u001b[0m                                      engine=engine))\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anantgupta/.conda/envs/python2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, **kwds)\u001b[0m\n\u001b[1;32m   2087\u001b[0m         \u001b[0;31m# infer column indices from self.usecols if it is specified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2088\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_col_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2089\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_original_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2091\u001b[0m         \u001b[0;31m# Now self.columns has the set of columns that we will process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anantgupta/.conda/envs/python2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_infer_columns\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2357\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2359\u001b[0;31m                     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffered_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m                     \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline_pos\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mhr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anantgupta/.conda/envs/python2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_buffered_line\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2528\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2530\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2532\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_for_bom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_row\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anantgupta/.conda/envs/python2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_next_line\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2630\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskipfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2631\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2632\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2634\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: ',' expected after '\"'"
     ]
    }
   ],
   "source": [
    "# Read all the news data\n",
    "# GNF DATA\n",
    "#!head -5  /home/ubuntu/anant/SAProject/data/kaggle/GNF/file1aa\n",
    "import pandas as pd\n",
    "import os\n",
    "GNFData=pd.DataFrame(columns=['publish_time','feed_code','source_url','headline_text'])\n",
    "for curFile in os.listdir('../data/kaggle/GNF/'):\n",
    "    print(curFile)\n",
    "    curData=pd.read_csv('../data/kaggle/GNF/{}'.format(curFile),skiprows=1,engine='python',sep=',',skip_blank_lines=True,error_bad_lines=False)\n",
    "    #curData=pd.read_csv('../data/kaggle/GNF/{}'.format(curFile),skiprows=1,error_bad_lines=False)\n",
    "    curData.columns=['publish_time','feed_code','source_url','headline_text']\n",
    "    for curcol in curData.columns:\n",
    "        curData[curcol]=curData[curcol].map(lambda x : ''.join([y for y in str(x) if ord(y) <= 126 and ord(y) >= 32]))\n",
    "    GNFData=pd.concat([GNFData,curData])\n",
    "    print(GNFData.shape)\n",
    "#GNFData.to_csv('../data/GNFHeadLineData.txt',sep=chr(255),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Expansion Algorithm\n",
    "#############################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "companyData=pd.read_csv('../data/CompanyDataSansLocationFull.txt')\n",
    "#companyData=pd.read_csv('/home/anantgupta/Documents/Programming/Projects/situationalAwareness/SAProject/CompanyDataSansLocationFull.txt')\n",
    "\n",
    "products=companyData[companyData['propertyLabel']=='products']['valueLabel'].unique()\n",
    "companies=companyData['CompanyLabel'].unique()\n",
    "\n",
    "def processCompany(x):\n",
    "    # We need to first convert all the non alphanumeric to ''\n",
    "    x=str(x).replace('\"','')\n",
    "    x=re.sub(r'\\([^)]*\\)', '',x)\n",
    "    if(len(x.strip()) <= 3):\n",
    "        x=''\n",
    "    return(x.strip())\n",
    "\n",
    "companyData['companyLabelProcessed']=companyData['CompanyLabel'].map(lambda x : processCompany(x))\n",
    "companyData.to_csv('../data/CompanyDataSansLocationFull.txt',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "def getContinousBigrams(inpArray):\n",
    "    outArray=[]\n",
    "    for curElement in inpArray:\n",
    "        tempArray=curElement.split(' ')\n",
    "        if(len(tempArray) > 2):\n",
    "            tempArray\n",
    "        else:\n",
    "            outArray.append(curElement)\n",
    "\n",
    "def getDetailedProducts(x,allElements):\n",
    "    indItems=x.split(',')\n",
    "    if(len(indItems) > 1):\n",
    "        indItems=[ ' '.join([str(y).strip().lower() for y in curItem.split(' ') if str(y).lower() not in stopWords]).strip() for curItem in indItems]\n",
    "        # Now for all the individual items, we will prepare a list of words that we need to search based on importance\n",
    "        bigrams = [' '.join(b) for l in indItems for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
    "        allElements[x]=bigrams\n",
    "        #allElements.extend(indItems)\n",
    "    else:\n",
    "        indItems=' '.join([str(y).strip().lower() for y in indItems[0].split(' ') if str(y).lower() not in stopWords])\n",
    "        bigrams = [' '.join(b) for l in [indItems] for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
    "        allElements[x]=bigrams\n",
    "        #allElements.append(indItems.strip())\n",
    "        \n",
    "allElements={}\n",
    "[getDetailedProducts(x,allElements) for x in products]\n",
    "#allElements=list(set(allElements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some papers\n",
    "#https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=2ahUKEwiz7vmB0abeAhXLWysKHVepAB8QFjAAegQICRAB&url=https%3A%2F%2Fstackoverflow.com%2Fquestions%2F21844546%2Fforming-bigrams-of-words-in-list-of-sentences-with-python&usg=AOvVaw1cF0_ZkiXJy-HXku5ozxhd\n",
    "#Discovering expansion entities for keyword-based entity search in linked data\n",
    "\n",
    "# Our approach\n",
    "# We will find the most commoinly occurring grammar linkages between the entity and aspect\n",
    "\n",
    "def cleanNewsText(x):\n",
    "    if(x[0:4]=='http'):\n",
    "        return ''\n",
    "    else:\n",
    "        return(''.join([y for y in str(x) if ord(y) <= 126 and ord(y) >= 32]))\n",
    "\n",
    "FullData['contentNew']=FullData['content'].map(lambda x : cleanNewsText(x))\n",
    "\n",
    "# Saving the DataFrame\n",
    "FullData.reset_index(inplace=True)\n",
    "FullData['contentNew']=FullData[['contentNew']].fillna('').values\n",
    "FullData=FullData[FullData['contentNew'] != '']\n",
    "FullData.to_csv('../../SADataCompany.txt',index=False)\n",
    "\n",
    "# Also saving the company Dataframe\n",
    "pd.DataFrame(companies,columns=['companyName']).to_csv('../../companyNameList.txt',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#FullData=pd.read_csv('../../SADataCompany.txt')\n",
    "companyData=pd.read_csv('../data/CompanyDataSansLocationFull.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#products=companyData[companyData['propertyLabel']=='products']['valueLabel'].unique()\n",
    "products=[]\n",
    "def getProducts(x,products):\n",
    "    products.extend([y.strip().lower() for y in x.split(',')])\n",
    "\n",
    "[getProducts(x,products) for x in companyData[companyData['propertyLabel']=='products']['valueLabel'].unique()]\n",
    "products=list(set(products))\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def convertLemma(x):\n",
    "    return(' '.join([wordnet_lemmatizer.lemmatize(y) for y in x.split(' ')]))\n",
    "\n",
    "products=[convertLemma(x) for x in products]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(422419, 9)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UCI NEWS\n",
    "#FullData=pd.read_csv('/home/anantgupta/Documents/Programming/Projects/situationalAwareness/SAProject/data/kaggle/uci-news-aggregator.csv')\n",
    "#FullData['indexVal']=FullData.reset_index().index.values\n",
    "FullData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Processing for the batch start index 0 \n",
      "Finished Processing for the batch start index 0Started Processing for the batch start index 100Started Processing for the batch start index 400Started Processing for the batch start index 300Started Processing for the batch start index 500Started Processing for the batch start index 600Started Processing for the batch start index 700 \n",
      "\n",
      "\n",
      "Started Processing for the batch start index 200\n",
      "\n",
      "\n",
      "\n",
      "Started Processing for the batch start index 800Started Processing for the batch start index 900 Started Processing for the batch start index 1000Started Processing for the batch start index 1300Started Processing for the batch start index 1200   Started Processing for the batch start index 1700Started Processing for the batch start index 1800\n",
      "Finished Processing for the batch start index 100Finished Processing for the batch start index 400Started Processing for the batch start index 1900\n",
      "Finished Processing for the batch start index 300Finished Processing for the batch start index 500Finished Processing for the batch start index 600\n",
      "\n",
      "\n",
      "Started Processing for the batch start index 1100\n",
      "\n",
      "\n",
      "Started Processing for the batch start index 1400Started Processing for the batch start index 1600Started Processing for the batch start index 1500Finished Processing for the batch start index 1700\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 200\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 1800Finished Processing for the batch start index 800Finished Processing for the batch start index 900\n",
      "Finished Processing for the batch start index 1000 Finished Processing for the batch start index 1200\n",
      "\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 1500 Finished Processing for the batch start index 1900Finished Processing for the batch start index 1400\n",
      "\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 1100\n",
      "Finished Processing for the batch start index 1300\n",
      "\n",
      "Finished Processing for the batch start index 1600\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 700Completed for the following batch of 20 threads 0 in 92.6840610504 seconds\n",
      "Started Processing for the batch start index 2000Started Processing for the batch start index 2100\n",
      "\n",
      "\n",
      "Started Processing for the batch start index 2200\n",
      "Finished Processing for the batch start index 2100Finished Processing for the batch start index 2000\n",
      "\n",
      " \n",
      "\n",
      "Started Processing for the batch start index 2300Finished Processing for the batch start index 2200Started Processing for the batch start index 2400Started Processing for the batch start index 2500Started Processing for the batch start index 2700Started Processing for the batch start index 2600\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 2300\n",
      "Finished Processing for the batch start index 2500Started Processing for the batch start index 2800Started Processing for the batch start index 2900 Finished Processing for the batch start index 2700Finished Processing for the batch start index 2600\n",
      "Started Processing for the batch start index 3000Finished Processing for the batch start index 2400\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 2900Finished Processing for the batch start index 2800\n",
      "\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 3000Started Processing for the batch start index 3100\n",
      "Started Processing for the batch start index 3200Started Processing for the batch start index 3300  Started Processing for the batch start index 3600Started Processing for the batch start index 3800Started Processing for the batch start index 3700 \n",
      "Finished Processing for the batch start index 3100Started Processing for the batch start index 3900\n",
      "\n",
      "Started Processing for the batch start index 3500\n",
      "\n",
      "\n",
      "Completed for the following batch of 20 threads 20 in 93.9670259953 seconds\n",
      "\n",
      "Finished Processing for the batch start index 3200Finished Processing for the batch start index 3300Started Processing for the batch start index 3400\n",
      "Finished Processing for the batch start index 3600Finished Processing for the batch start index 3800Finished Processing for the batch start index 3700Started Processing for the batch start index 4000 Started Processing for the batch start index 4200 Started Processing for the batch start index 4400 Started Processing for the batch start index 4600Started Processing for the batch start index 4700Started Processing for the batch start index 4800\n",
      "\n",
      "Finished Processing for the batch start index 3900\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 3500\n",
      "\n",
      "\n",
      "\n",
      "Started Processing for the batch start index 4100\n",
      "Started Processing for the batch start index 4300Started Processing for the batch start index 5200\n",
      "Started Processing for the batch start index 4500\n",
      "\n",
      "Started Processing for the batch start index 4900Started Processing for the batch start index 5000Started Processing for the batch start index 5100Started Processing for the batch start index 5300Started Processing for the batch start index 5400Started Processing for the batch start index 5600  Started Processing for the batch start index 5700Finished Processing for the batch start index 4800\n",
      "Started Processing for the batch start index 5500\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 4000Finished Processing for the batch start index 4200\n",
      "\n",
      "Finished Processing for the batch start index 4400\n",
      "Finished Processing for the batch start index 4600Finished Processing for the batch start index 4700\n",
      "\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 5000\n",
      "Finished Processing for the batch start index 5600Finished Processing for the batch start index 3400 Finished Processing for the batch start index 5400  \n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 4300Finished Processing for the batch start index 5200\n",
      "Finished Processing for the batch start index 4500\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 4100Finished Processing for the batch start index 5100\n",
      "Finished Processing for the batch start index 4900\n",
      "Finished Processing for the batch start index 5300\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Started Processing for the batch start index 5800\n",
      "Finished Processing for the batch start index 5700Started Processing for the batch start index 5900 Completed for the following batch of 20 threads 40 in 93.1456770897 secondsFinished Processing for the batch start index 5500\n",
      "\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 5900 Finished Processing for the batch start index 5800\n",
      "\n",
      "\n",
      "Started Processing for the batch start index 6000 Started Processing for the batch start index 6200 \n",
      "Started Processing for the batch start index 6100\n",
      "Started Processing for the batch start index 6400Started Processing for the batch start index 6300Finished Processing for the batch start index 6000\n",
      "Finished Processing for the batch start index 6200\n",
      "  \n",
      "Finished Processing for the batch start index 6100\n",
      "Finished Processing for the batch start index 6400Started Processing for the batch start index 6500Started Processing for the batch start index 6600Started Processing for the batch start index 6700  Started Processing for the batch start index 7000Finished Processing for the batch start index 6300\n",
      "\n",
      "Started Processing for the batch start index 7200\n",
      "\n",
      "\n",
      "\n",
      " Started Processing for the batch start index 6800Started Processing for the batch start index 6900\n",
      "Started Processing for the batch start index 7400 \n",
      "Started Processing for the batch start index 7300\n",
      "\n",
      " Finished Processing for the batch start index 6500Finished Processing for the batch start index 6600Finished Processing for the batch start index 6700Started Processing for the batch start index 7100\n",
      "\n",
      "Finished Processing for the batch start index 7000Started Processing for the batch start index 7600Started Processing for the batch start index 7700Started Processing for the batch start index 7800Started Processing for the batch start index 7900\n",
      "\n",
      "Finished Processing for the batch start index 7400Finished Processing for the batch start index 7200Started Processing for the batch start index 7500\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 6800Finished Processing for the batch start index 6900\n",
      "\n",
      "\n",
      "\n",
      "Completed for the following batch of 20 threads 60 in 95.11414814 secondsStarted Processing for the batch start index 8000Started Processing for the batch start index 8100Started Processing for the batch start index 8200 Started Processing for the batch start index 8500 Finished Processing for the batch start index 7900Finished Processing for the batch start index 7300\n",
      "\n",
      "\n",
      "Started Processing for the batch start index 8400\n",
      "\n",
      "Finished Processing for the batch start index 7100\n",
      "\n",
      "Started Processing for the batch start index 8300Finished Processing for the batch start index 7600Finished Processing for the batch start index 7700Finished Processing for the batch start index 7800\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Processing for the batch start index 8600Started Processing for the batch start index 8700 Started Processing for the batch start index 9000    Started Processing for the batch start index 9400Started Processing for the batch start index 9600 \n",
      "\n",
      "Started Processing for the batch start index 9700Started Processing for the batch start index 9500Finished Processing for the batch start index 7500\n",
      "\n",
      "Finished Processing for the batch start index 8500\n",
      "Started Processing for the batch start index 9800\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Started Processing for the batch start index 9300Finished Processing for the batch start index 8000\n",
      "Started Processing for the batch start index 9200\n",
      "\n",
      "Started Processing for the batch start index 8800\n",
      "Started Processing for the batch start index 9100Started Processing for the batch start index 8900\n",
      "\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 8400 \n",
      "Finished Processing for the batch start index 9000\n",
      "Finished Processing for the batch start index 9400Finished Processing for the batch start index 8300\n",
      "Finished Processing for the batch start index 8700\n",
      "\n",
      "Finished Processing for the batch start index 8100\n",
      "Finished Processing for the batch start index 8600Started Processing for the batch start index 9900Finished Processing for the batch start index 8200\n",
      "Finished Processing for the batch start index 9100Finished Processing for the batch start index 9700Finished Processing for the batch start index 9500\n",
      "\n",
      "Finished Processing for the batch start index 9600\n",
      "\n",
      "Finished Processing for the batch start index 9800\n",
      "\n",
      "Finished Processing for the batch start index 8800\n",
      "Completed for the following batch of 20 threads 80 in 92.7581179142 secondsFinished Processing for the batch start index 9300\n",
      "\n",
      "Finished Processing for the batch start index 9200Finished Processing for the batch start index 8900\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Finished Processing for the batch start index 9900\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#FullData.head(5)\n",
    "#np.range(FullData.shape[0],1000)\n",
    "#np.arange(1,FullData.shape[0],1000)\n",
    "#df1, df2 = np.split(df, [72], axis=1)\n",
    "\n",
    "import time\n",
    "from threading import Thread\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "companies=companyData['companyLabelProcessed'].values\n",
    "companies=[str(x) for x in companies]\n",
    "\n",
    "# This method is very naive, as it does not make the split checks, but for now we will go ahead with this only\n",
    "# This clearly requires refinement\n",
    "results=[]\n",
    "def findCompanies(x):\n",
    "    if len(x) > 10:\n",
    "        matches=list(set([y for y in companies if y in x]))\n",
    "        return([str(curMatch) for curMatch in matches if len(str(curMatch)) > 1])\n",
    "    else:\n",
    "        return([''])\n",
    "\n",
    "def getNewsBatch(batchIndex,curList,results):\n",
    "    print \"Started Processing for the batch start index {}\".format(batchIndex)\n",
    "    results.append([[x[0],findCompanies(str(x[1]))] for x in curList])\n",
    "    print \"Finished Processing for the batch start index {}\".format(batchIndex)\n",
    "\n",
    "def calculateParallel(numbers, threads=2):\n",
    "    pool = ThreadPool(100)\n",
    "    results = pool.map(getNewsBatch, numbers)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return results\n",
    "    \n",
    "threadList=[]\n",
    "#for i in range(0,143000,100):\n",
    "#for i in range(14300,143000,100):\n",
    "for i in range(0,10000,100):\n",
    "    batchIndex=i\n",
    "    curList=FullData[i:i+100][['indexVal','TITLE']].values\n",
    "    threadList.append(Thread(target=getNewsBatch, args=(batchIndex,curList,results,)))\n",
    "\n",
    "#for curIter in range(0,1430-143,20):\n",
    "for curIter in range(0,100,20):\n",
    "    startTime=time.time()\n",
    "    curThreadList=threadList[curIter:curIter + 20]\n",
    "    for curThread in curThreadList:\n",
    "        curThread.start()\n",
    "    for curThread in curThreadList:\n",
    "        curThread.join()\n",
    "    print(\"Completed for the following batch of 20 threads {} in {} seconds\".format(curIter,str(time.time() - startTime)))\n",
    "    \n",
    "resultsPD=[]\n",
    "for x in results:\n",
    "    for y in x:\n",
    "        resultsPD.append(y)\n",
    "resultsPD=pd.DataFrame(resultsPD,columns=['index','companyMatch'])\n",
    "resultsPD['lengthVal']=resultsPD['companyMatch'].map(lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resultsPD.to_csv('../../companyMatches.txt',index=False)\n",
    "#FullData[['index','contentNew']].merge(resultsPD[['index','companyMatch']],left_on='index',right_on='index',how='inner').head(5)\n",
    "#resultsPD[resultsPD['lengthVal'] > 0]\n",
    "#FullData[['index','contentNew']].merge(resultsPD[['index','companyMatch']],left_on='index',right_on='index',how='inner').head(5)\n",
    "#FullData[FullData['index']==1]\n",
    "FullData.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FullData['index']=FullData['index'].values.astype(int)\n",
    "#FullData['index'].values\n",
    "#FullData[['index','contentNew']].merge(resultsPD[['index','companyMatch']],left_on='index',right_on='index',how='inner').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsPD=resultsPD.drop_duplicates()\n",
    "FullData['index']=FullData['index'].values.astype(int)\n",
    "\n",
    "# Merging the data\n",
    "workingData=FullData.merge(resultsPD,left_on='index',right_on='index',how='inner')\n",
    "\n",
    "# Writing the data\n",
    "workingData.to_csv('../../workingData.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from threading import Thread\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "# PRODUCTS MATCHING\n",
    "products=[str(x) for x in products if len(str(x)) > 3]\n",
    "\n",
    "# This method is very naive, as it does not make the split checks, but for now we will go ahead with this only\n",
    "# This clearly requires refinement\n",
    "results=[]\n",
    "def findProducts(x):\n",
    "    if len(x) > 10:\n",
    "        matches=list(set([y for y in products if y in x]))\n",
    "        return([str(curMatch) for curMatch in matches if len(str(curMatch)) > 1])\n",
    "    else:\n",
    "        return([''])\n",
    "\n",
    "def getNewsBatch(batchIndex,curList,results):\n",
    "    print \"Started Processing for the batch start index {}\".format(batchIndex)\n",
    "    results.append([[x[0],findProducts(str(x[1]))] for x in curList])\n",
    "    print \"Finished Processing for the batch start index {}\".format(batchIndex)\n",
    "\n",
    "def calculateParallel(numbers, threads=2):\n",
    "    pool = ThreadPool(100)\n",
    "    results = pool.map(getNewsBatch, numbers)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return results\n",
    "    \n",
    "threadList=[]\n",
    "#for i in range(0,143000,100):\n",
    "#for i in range(14300,143000,100):\n",
    "for i in range(0,14300,100):\n",
    "    batchIndex=i\n",
    "    curList=FullData[i:i+100][['index','contentNew']].values\n",
    "    threadList.append(Thread(target=getNewsBatch, args=(batchIndex,curList,results,)))\n",
    "\n",
    "#for curIter in range(0,1430-143,20):\n",
    "for curIter in range(0,1430,20):\n",
    "    startTime=time.time()\n",
    "    curThreadList=threadList[curIter:curIter + 20]\n",
    "    for curThread in curThreadList:\n",
    "        curThread.start()\n",
    "    for curThread in curThreadList:\n",
    "        curThread.join()\n",
    "    print(\"Completed for the following batch of 50 threads {} in {} seconds\".format(curIter,str(time.time() - startTime)))\n",
    "\n",
    "resultsPD1=[]\n",
    "for x in results:\n",
    "    for y in x:\n",
    "        resultsPD1.append(y)\n",
    "resultsPD1=pd.DataFrame(resultsPD1,columns=['index','productMatch'])\n",
    "resultsPD1=resultsPD1.drop_duplicates()\n",
    "resultsPD1.to_csv('../../productMatches.txt',index=False)\n",
    "# Read back the data with the company Filter\n",
    "#workingData=pd.read_csv('../../workingData.csv')\n",
    "\n",
    "# Merging the data with the product match\n",
    "#workingData=workingData.merge(resultsPD,left_on='index',right_on='index',how='inner')\n",
    "\n",
    "# Writing the data back \n",
    "#workingData.to_csv('../../workingDataWithProducts.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsPD1.to_csv('../../productMatches.txt',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsPD=[]\n",
    "for x in results:\n",
    "    for y in x:\n",
    "        resultsPD.append(y)\n",
    "resultsPD=pd.DataFrame(resultsPD,columns=['index','productMatch'])\n",
    "workingData=pd.read_csv('../../workingData.csv')\n",
    "\n",
    "# Merging the data\n",
    "workingData=workingData.merge(resultsPD,left_on='index',right_on='index',how='inner')\n",
    "\n",
    "# Writing the data\n",
    "workingData.to_csv('../../workingDataWithProducts.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "import requests\n",
    "import urllib\n",
    "import sys\n",
    "\n",
    "def getSubjects(phrase):\n",
    "    phrase=\"\\\"\"+phrase+\"\\\"\"\n",
    "    query=\"\"\"PREFIX dbr: <http://dbpedia.org/resource/>\n",
    "    PREFIX bif:<>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "    PREFIX rdf:<http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX dbpedia-owl: <http://dbpedia.org/ontology/>\n",
    "    PREFIX foaf: <http://xmlns.com/foaf/0.1/> \n",
    "    SELECT DISTINCT * WHERE \n",
    "    {\n",
    "     ?s rdfs:label ?label .\n",
    "     ?sub ?p ?s .\n",
    "     FILTER (CONTAINS(lcase(?label),\"\"\"+phrase+\"\"\"))\n",
    "    }\"\"\"\n",
    "    #print(query)\n",
    "    q=urllib.parse.quote_plus(query)\n",
    "    #print(queryLocalDB(q))\n",
    "    return queryLocalDB(q)\n",
    "\n",
    "def queryLocalDB(q):\n",
    "    predicateList=[]\n",
    "    with open(r'C:\\Users\\Administrator\\Scripts_Technophilia\\predicateList1.txt') as f:\n",
    "        content = f.readlines()\n",
    "    for x in content:\n",
    "        x=x[1:-2]\n",
    "        #print(x)\n",
    "        predicateList.append(x)\n",
    "    #predicateList = [x.strip() for x in content] \n",
    "    #print(predicateList)\n",
    "    url = \"http://172.16.131.14:3030/da/query\"\n",
    "    #print(q)\n",
    "    \n",
    "    payload = \"query=\"+q\n",
    "    headers = {\n",
    "        'Content-Type': \"application/x-www-form-urlencoded\",\n",
    "        'cache-control': \"no-cache\",\n",
    "        'Postman-Token': \"681d8312-7fab-40ab-b91d-e1926ad703cd\"\n",
    "        }\n",
    "\n",
    "    response = requests.request(\"POST\", url, data=payload, headers=headers)\n",
    "#     print(response.text)\n",
    "#     print(response.json().keys())\n",
    "#     print(response.json()['results']['bindings'])\n",
    "#     print('I am in query function')\n",
    "#     print(len(response.json()['results']['bindings']))\n",
    "    subjects=[]\n",
    "    superSubjects=[]\n",
    "    for i in response.json()['results']['bindings']:\n",
    "        #print('here we go')\n",
    "        #print(i['s']['value'])\n",
    "        #print(i['sub']['value'])\n",
    "        #print(i['p']['value'])\n",
    "        if i['p']['value'] in predicateList:\n",
    "            #print('hello, hi')\n",
    "            #print(i['p']['value'])\n",
    "            subjects.append(i['s']['value'])\n",
    "            superSubjects.append(i['sub']['value'])\n",
    "    #print('subjects')\n",
    "    #print(subjects)\n",
    "    return subjects,superSubjects\n",
    "\n",
    "#to preprocess query :\n",
    "# stop words removal\n",
    "# NER\n",
    "#Nouns phrases\n",
    "#nouns\n",
    "#verbs\n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "\t\n",
    "def parseChunking(parser, postags):\n",
    "    chunked = parser.parse(postags)\n",
    "#     print(type(chunked))\n",
    "#     print(type(postags))\n",
    "    for subtree in chunked.subtrees():\n",
    "        print(subtree)\n",
    "    phrases=[]\n",
    "    for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
    "        np=''\n",
    "#         print(subtree)\n",
    "        for s in subtree:\n",
    "#             print(s[0], s[1])\n",
    "            np=np+s[0]+' '\n",
    "        phrases.append(np.rstrip())    \n",
    "    return phrases\n",
    "    \n",
    "\n",
    "def queryPreprocessing(query):\n",
    "    phraseList=[]\n",
    "    tokens = word_tokenize(query)\n",
    "    postags=nltk.pos_tag(tokens)\n",
    "#     print(postags)\n",
    "    #chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "    chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NN>+<NNS>?}\"\"\"\n",
    "    chunkGram2 = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "    chunkParser = nltk.RegexpParser(chunkGram)\n",
    "    chunkParser2 = nltk.RegexpParser(chunkGram2)\n",
    "    #chunked = chunkParser.parse(postags)\n",
    "    phraseList.extend(parseChunking(chunkParser,postags))\n",
    "    phraseList.extend(parseChunking(chunkParser2,postags))\n",
    "    \n",
    "    #print(phraseList)\n",
    "    return phraseList\n",
    "        \n",
    "    \n",
    "def main():\n",
    "    phrases =queryPreprocessing('services industry in america')\n",
    "    subjects=[]\n",
    "    print(phrases)\n",
    "    for phrase in phrases:\n",
    "        if len(phrase)>3:\n",
    "            print(phrase)\n",
    "            subjects, superSubjects=getSubjects(phrase.lower())\n",
    "        #subjects.append(getSubjects(phrase.lower()))\n",
    "    print(len(set(subjects)))\n",
    "    print(len(set(superSubjects)))\n",
    "   \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
